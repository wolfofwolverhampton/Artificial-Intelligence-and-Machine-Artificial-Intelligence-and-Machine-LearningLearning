{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "XykmB25tHQwh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3592bd-0eb8-4e49-8295-f3247d1b94ed"
      },
      "id": "XykmB25tHQwh",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "bc289888-2564-4a7a-84b2-7fd745f70a4a",
      "metadata": {
        "id": "bc289888-2564-4a7a-84b2-7fd745f70a4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "f9843b0e-303e-40f1-b015-6e65b7604d1e",
      "metadata": {
        "id": "f9843b0e-303e-40f1-b015-6e65b7604d1e"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9/npi.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "3da4a68a-55fc-4f53-bd94-4d7c7a607745",
      "metadata": {
        "id": "3da4a68a-55fc-4f53-bd94-4d7c7a607745"
      },
      "outputs": [],
      "source": [
        "root_path = \"/content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "63c80ade-4f17-48c5-95e9-60d15bd23b29",
      "metadata": {
        "id": "63c80ade-4f17-48c5-95e9-60d15bd23b29",
        "outputId": "adc4786e-aee2-4be5-cbc1-351243cf6b8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ CSV file already exists at: /content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9/raw_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Read the .txt file into a DataFrame\n",
        "lines = pd.read_table(data_path, names=['source', 'target', 'comments'])\n",
        "\n",
        "# Drop the 'comments' column if it exists\n",
        "if 'comments' in lines.columns:\n",
        "    lines = lines.drop(columns=['comments'])\n",
        "\n",
        "# Path to save the cleaned CSV file\n",
        "csv_path = os.path.join(\"/content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9\", \"raw_data.csv\")\n",
        "\n",
        "# Save as CSV if it doesn't already exist\n",
        "if not os.path.exists(csv_path):\n",
        "    lines.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "    print(f\"✅ CSV file saved to: {csv_path}\")\n",
        "else:\n",
        "    print(f\"⚠️ CSV file already exists at: {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "94b403c4-b3ed-4e1d-b3d8-424ec82223d9",
      "metadata": {
        "id": "94b403c4-b3ed-4e1d-b3d8-424ec82223d9",
        "outputId": "1b0f29a7-26e6-452d-af01-f5a854ebed22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                source  \\\n",
              "1021          I'm also from Australia.   \n",
              "1584     The attackers escaped easily.   \n",
              "1644    I don't have a microwave oven.   \n",
              "1344       Can you give me an example?   \n",
              "413                  What did you say?   \n",
              "1893  You need to respect the elderly.   \n",
              "\n",
              "                                         target  \n",
              "1021                म पनि अस्ट्रेलिया बाट हुँ ।  \n",
              "1584                  आक्रमणकारीहरू सजिलै भागे।  \n",
              "1644                       मसँग माइक्रोवेभ छैन।  \n",
              "1344  के तपाईं मलाई एउटा उदाहरण दिन सक्नुहुन्छ?  \n",
              "413                       तपाईले के भन्नु भयो ?  \n",
              "1893        ज्येष्ठ नागरिकलाई सम्मान गर्नुपर्छ।  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4630faed-377a-4300-a534-47a859ff71db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1021</th>\n",
              "      <td>I'm also from Australia.</td>\n",
              "      <td>म पनि अस्ट्रेलिया बाट हुँ ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1584</th>\n",
              "      <td>The attackers escaped easily.</td>\n",
              "      <td>आक्रमणकारीहरू सजिलै भागे।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1644</th>\n",
              "      <td>I don't have a microwave oven.</td>\n",
              "      <td>मसँग माइक्रोवेभ छैन।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1344</th>\n",
              "      <td>Can you give me an example?</td>\n",
              "      <td>के तपाईं मलाई एउटा उदाहरण दिन सक्नुहुन्छ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>What did you say?</td>\n",
              "      <td>तपाईले के भन्नु भयो ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893</th>\n",
              "      <td>You need to respect the elderly.</td>\n",
              "      <td>ज्येष्ठ नागरिकलाई सम्मान गर्नुपर्छ।</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4630faed-377a-4300-a534-47a859ff71db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4630faed-377a-4300-a534-47a859ff71db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4630faed-377a-4300-a534-47a859ff71db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-36995c39-2cf6-4e5b-a558-c0859c9f8ad5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36995c39-2cf6-4e5b-a558-c0859c9f8ad5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-36995c39-2cf6-4e5b-a558-c0859c9f8ad5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"raw_data\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"I'm also from Australia.\",\n          \"The attackers escaped easily.\",\n          \"You need to respect the elderly.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"\\u092e \\u092a\\u0928\\u093f \\u0905\\u0938\\u094d\\u091f\\u094d\\u0930\\u0947\\u0932\\u093f\\u092f\\u093e \\u092c\\u093e\\u091f \\u0939\\u0941\\u0901 \\u0964\",\n          \"\\u0906\\u0915\\u094d\\u0930\\u092e\\u0923\\u0915\\u093e\\u0930\\u0940\\u0939\\u0930\\u0942 \\u0938\\u091c\\u093f\\u0932\\u0948 \\u092d\\u093e\\u0917\\u0947\\u0964\",\n          \"\\u091c\\u094d\\u092f\\u0947\\u0937\\u094d\\u0920 \\u0928\\u093e\\u0917\\u0930\\u093f\\u0915\\u0932\\u093e\\u0908 \\u0938\\u092e\\u094d\\u092e\\u093e\\u0928 \\u0917\\u0930\\u094d\\u0928\\u0941\\u092a\\u0930\\u094d\\u091b\\u0964\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "raw_data = pd.read_csv(\"/content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9/raw_data.csv\")\n",
        "raw_data.sample(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0caf1e33-25ad-4199-9510-9261d48ae41c",
      "metadata": {
        "id": "0caf1e33-25ad-4199-9510-9261d48ae41c"
      },
      "source": [
        "# Clean, Normalize and Prepare Target Sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "6d38e637-03ec-4c18-b36c-0ed54befe357",
      "metadata": {
        "id": "6d38e637-03ec-4c18-b36c-0ed54befe357"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "from string import digits\n",
        "\n",
        "def clean_text_data(df, output_path=None):\n",
        "    \"\"\"\n",
        "    Cleans source and target text columns in a DataFrame for translation tasks.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        A DataFrame with 'source' and 'target' columns.\n",
        "        - 'source': English sentences\n",
        "        - 'target': Translated Nepali sentences\n",
        "\n",
        "    output_path : str, optional\n",
        "        If provided, saves the cleaned DataFrame as CSV with 'cleaned_source' and 'cleaned_target'.\n",
        "        Will not overwrite if file already exists.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        Cleaned data with minimal changes to preserve sentence meaning.\n",
        "    \"\"\"\n",
        "\n",
        "    # Lowercase both columns\n",
        "    df.source = df.source.apply(lambda x: x.lower())\n",
        "    df.target = df.target.apply(lambda x: x.lower())\n",
        "\n",
        "    # Remove stray apostrophes or quotes\n",
        "    df.source = df.source.apply(lambda x: re.sub(r\"[\\\"’‘“”']\", '', x))\n",
        "    df.target = df.target.apply(lambda x: re.sub(r\"[\\\"’‘“”']\", '', x))\n",
        "\n",
        "    # Remove digits only\n",
        "    df.source = df.source.apply(lambda x: re.sub(r\"\\d+\", '', x))\n",
        "    df.target = df.target.apply(lambda x: re.sub(r\"\\d+\", '', x))\n",
        "\n",
        "    # Normalize whitespace\n",
        "    df.source = df.source.apply(lambda x: re.sub(r\"\\s+\", \" \", x.strip()))\n",
        "    df.target = df.target.apply(lambda x: re.sub(r\"\\s+\", \" \", x.strip()))\n",
        "\n",
        "    # Add START_ and _END to target text\n",
        "    df.target = df.target.apply(lambda x: f\"START_ {x} _END\")\n",
        "\n",
        "    # Rename the cleaned columns\n",
        "    df.rename(columns={\"source\": \"cleaned_source\", \"target\": \"cleaned_target\"}, inplace=True)\n",
        "\n",
        "    # Save cleaned file (if not exists)\n",
        "    if output_path:\n",
        "        if not os.path.exists(output_path):\n",
        "            df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"✅ Cleaned data saved to: {output_path}\")\n",
        "        else:\n",
        "            print(f\"⚠️ File already exists. Skipping save: {output_path}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "fe285f48-ff28-4269-899b-19106d636ecd",
      "metadata": {
        "id": "fe285f48-ff28-4269-899b-19106d636ecd",
        "outputId": "0a4da562-4331-44da-e068-08b3fb23fca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ File already exists. Skipping save: /content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9/cleaned_data_translation.csv\n",
            "Index(['cleaned_source', 'cleaned_target'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "cleaned_data_path = os.path.join(root_path, \"cleaned_data_translation.csv\")\n",
        "cleaned_lines = clean_text_data(raw_data, cleaned_data_path)\n",
        "# Check the column names\n",
        "print(cleaned_lines.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da2fa26-a555-4750-96c1-d8c3a98af81c",
      "metadata": {
        "id": "5da2fa26-a555-4750-96c1-d8c3a98af81c"
      },
      "source": [
        "### For sanity - ReLoad the Cleaned Dataset.\n",
        "\n",
        "For consistency, we reload the dataset from the saved CSV. The earlier extraction and saving steps will not be rerun moving forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "5da54267-d351-4843-b0c1-b31b11c8a15d",
      "metadata": {
        "id": "5da54267-d351-4843-b0c1-b31b11c8a15d"
      },
      "outputs": [],
      "source": [
        "cleaned_data = pd.read_csv(\"/content/drive/MyDrive/2025 - 6CS012 - AI and ML - Student/Week9/cleaned_data_translation.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "34d540bb-4816-4dc0-b778-07aeb52737d5",
      "metadata": {
        "id": "34d540bb-4816-4dc0-b778-07aeb52737d5",
        "outputId": "1d9d83df-a84d-4887-aca2-6c49ae1b3e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                cleaned_source  \\\n",
              "2360  tom is all alone with no one to talk to.   \n",
              "2153       tom doesnt like milk in his coffee.   \n",
              "2398  without me, you wont be able to do that.   \n",
              "1871          tom is still too young to drive.   \n",
              "1474               id rather die than give up.   \n",
              "1206                he used to be a gentleman.   \n",
              "\n",
              "                                         cleaned_target  \n",
              "2360   START_ टम एक्लै छ जससँग कुरा गर्न कोही छैन। _END  \n",
              "2153       START_ टमलाई आफ्नो कफीमा दूध मन पर्दैन। _END  \n",
              "2398  START_ म बिना, तपाईं त्यो गर्न सक्षम हुनुहुने ...  \n",
              "1871  START_ टम अझै पनि ड्राइभ गर्न को लागी धेरै जवा...  \n",
              "1474     START_ म हार मान्नु भन्दा मर्न रुचाउँछु । _END  \n",
              "1206         START_ उहाँ पहिले सज्जन हुनुहुन्थ्यो। _END  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d63325e7-e56e-4497-affd-3a295c41db8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cleaned_source</th>\n",
              "      <th>cleaned_target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2360</th>\n",
              "      <td>tom is all alone with no one to talk to.</td>\n",
              "      <td>START_ टम एक्लै छ जससँग कुरा गर्न कोही छैन। _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2153</th>\n",
              "      <td>tom doesnt like milk in his coffee.</td>\n",
              "      <td>START_ टमलाई आफ्नो कफीमा दूध मन पर्दैन। _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2398</th>\n",
              "      <td>without me, you wont be able to do that.</td>\n",
              "      <td>START_ म बिना, तपाईं त्यो गर्न सक्षम हुनुहुने ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1871</th>\n",
              "      <td>tom is still too young to drive.</td>\n",
              "      <td>START_ टम अझै पनि ड्राइभ गर्न को लागी धेरै जवा...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1474</th>\n",
              "      <td>id rather die than give up.</td>\n",
              "      <td>START_ म हार मान्नु भन्दा मर्न रुचाउँछु । _END</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1206</th>\n",
              "      <td>he used to be a gentleman.</td>\n",
              "      <td>START_ उहाँ पहिले सज्जन हुनुहुन्थ्यो। _END</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d63325e7-e56e-4497-affd-3a295c41db8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d63325e7-e56e-4497-affd-3a295c41db8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d63325e7-e56e-4497-affd-3a295c41db8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-59d13c4f-d6fb-495e-ae54-d4ddfb813a77\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-59d13c4f-d6fb-495e-ae54-d4ddfb813a77')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-59d13c4f-d6fb-495e-ae54-d4ddfb813a77 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"cleaned_data\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"cleaned_source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"tom is all alone with no one to talk to.\",\n          \"tom doesnt like milk in his coffee.\",\n          \"he used to be a gentleman.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_target\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"START_ \\u091f\\u092e \\u090f\\u0915\\u094d\\u0932\\u0948 \\u091b \\u091c\\u0938\\u0938\\u0901\\u0917 \\u0915\\u0941\\u0930\\u093e \\u0917\\u0930\\u094d\\u0928 \\u0915\\u094b\\u0939\\u0940 \\u091b\\u0948\\u0928\\u0964 _END\",\n          \"START_ \\u091f\\u092e\\u0932\\u093e\\u0908 \\u0906\\u092b\\u094d\\u0928\\u094b \\u0915\\u092b\\u0940\\u092e\\u093e \\u0926\\u0942\\u0927 \\u092e\\u0928 \\u092a\\u0930\\u094d\\u0926\\u0948\\u0928\\u0964 _END\",\n          \"START_ \\u0909\\u0939\\u093e\\u0901 \\u092a\\u0939\\u093f\\u0932\\u0947 \\u0938\\u091c\\u094d\\u091c\\u0928 \\u0939\\u0941\\u0928\\u0941\\u0939\\u0941\\u0928\\u094d\\u0925\\u094d\\u092f\\u094b\\u0964 _END\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "cleaned_data.sample(6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "028f22a4-c599-42a0-b018-e26abf36160b",
      "metadata": {
        "id": "028f22a4-c599-42a0-b018-e26abf36160b"
      },
      "source": [
        "### Vocabulary Extractions:\n",
        "\n",
        "We put all the words from source[English] to a list called source vocabulary.\n",
        "\n",
        "We put all the words from target[Nepali] to a list called target vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "3cb192b4-097e-45dc-991d-8ae7302b84a7",
      "metadata": {
        "id": "3cb192b4-097e-45dc-991d-8ae7302b84a7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "5001a75c-aa05-4d54-bf1e-b00abd038e0b",
      "metadata": {
        "id": "5001a75c-aa05-4d54-bf1e-b00abd038e0b",
        "outputId": "f162d6a5-844b-45b1-a6a5-705ede07ee06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3269\n"
          ]
        }
      ],
      "source": [
        "all_source_words = set()\n",
        "for source in cleaned_data.cleaned_source:\n",
        "    for word in source.split():\n",
        "        all_source_words.add(word)\n",
        "\n",
        "all_target_words = set()\n",
        "for target in cleaned_data.cleaned_target:\n",
        "    for word in target.split():\n",
        "        all_target_words.add(word)\n",
        "\n",
        "source_words = sorted(list(all_source_words))\n",
        "target_words = sorted(list(all_target_words))\n",
        "print(len(target_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "199cdbfe-dd03-49b8-979a-62aee1ec2232",
      "metadata": {
        "id": "199cdbfe-dd03-49b8-979a-62aee1ec2232"
      },
      "source": [
        "### Sentence Length Calculation:\n",
        "\n",
        "Finding longest sentence both in Source and Target.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "81f9b50c-c6dc-4005-bbd4-899e1edd38f6",
      "metadata": {
        "id": "81f9b50c-c6dc-4005-bbd4-899e1edd38f6",
        "outputId": "e6318173-b745-4c10-d35c-930d7abc0647",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Max length of the source sentence 25\n",
            " Max length of the target sentence 22\n"
          ]
        }
      ],
      "source": [
        "#Find maximum sentence length in  the source and target data\n",
        "source_length_list=[]\n",
        "for l in cleaned_data.cleaned_source:\n",
        "    source_length_list.append(len(l.split(' ')))\n",
        "max_source_length= max(source_length_list)\n",
        "print(\" Max length of the source sentence\",max_source_length)\n",
        "target_length_list=[]\n",
        "for l in cleaned_data.cleaned_target:\n",
        "    target_length_list.append(len(l.split(' ')))\n",
        "max_target_length= max(target_length_list)\n",
        "print(\" Max length of the target sentence\",max_target_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef21812e-8764-4c05-b517-c8e8ecb825ee",
      "metadata": {
        "id": "ef21812e-8764-4c05-b517-c8e8ecb825ee"
      },
      "source": [
        "### Word - to - Index and Index - to - Word Mapping\n",
        "\n",
        "Creating a Look Up table.\n",
        "  1.   We create a dicitionary word2indx both for source and target.\n",
        "  2.   We will also Creata reverse dicitionary indx2word for both source and target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "b04ec74e-4e27-40d0-be0f-4bec94db65b8",
      "metadata": {
        "id": "b04ec74e-4e27-40d0-be0f-4bec94db65b8",
        "outputId": "ff1da3f8-baeb-4a34-8ea3-c6fcd7fb5760",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 0, '<UNK>': 1, ',': 2, '.': 3, ':': 4, ':.': 5, '?': 6, 'a': 7, 'able': 8, 'aboard.': 9, 'about': 10, 'about?': 11, 'above': 12, 'abroad': 13, 'abroad.': 14, 'abused': 15, 'accept': 16, 'accept.': 17, 'accepted': 18, 'accident.': 19, 'accountant?': 20, 'accurate': 21, 'accurately': 22, 'across': 23, 'act': 24, 'active.': 25, 'actor?': 26, 'actually': 27, 'actually,': 28, 'add?': 29, 'addict.': 30, 'address.': 31, 'admit': 32, 'admitted': 33, 'advance': 34, 'advance.': 35, 'adventures': 36, 'advice,': 37, 'advice.': 38, 'afraid': 39, 'after': 40, 'afternoon.': 41, 'afternoon?': 42, 'again': 43, 'again,': 44, 'again.': 45, 'again?': 46, 'age.': 47, 'aggressive.': 48, 'ago.': 49, 'agree': 50, 'agreed': 51, 'ahead': 52, 'airport': 53, 'alcohol?': 54, 'alice': 55, 'all': 56, 'all.': 57, 'all?': 58, 'allergic': 59, 'allergies.': 60, 'allowed': 61, 'almost': 62, 'alone': 63, 'alone!': 64, 'alone.': 65, 'alone?': 66, 'along': 67, 'already': 68, 'also': 69, 'alternative.': 70, 'always': 71, 'am': 72, 'am.': 73, 'amazing': 74, 'amazing.': 75, 'ambitious.': 76, 'amusing': 77, 'an': 78, 'ancient': 79, 'and': 80, 'angry.': 81, 'animal.': 82, 'animals': 83, 'animals.': 84, 'announced': 85, 'annoying': 86, 'another': 87, 'answer': 88, 'answer.': 89, 'answer?': 90, 'answered.': 91, 'any': 92, 'anybody': 93, 'anybody.': 94, 'anymore.': 95, 'anyone': 96, 'anyone.': 97, 'anything': 98, 'anything.': 99, 'anything?': 100, 'anyway.': 101, 'anyway?': 102, 'anywhere': 103, 'anywhere.': 104, 'apart.': 105, 'apartment': 106, 'apartment.': 107, 'apologize': 108, 'apologize.': 109, 'apologized.': 110, 'appearance.': 111, 'apple': 112, 'apple.': 113, 'apples,': 114, 'apples.': 115, 'application': 116, 'appreciate': 117, 'approved.': 118, 'are': 119, 'are!': 120, 'are.': 121, 'arent': 122, 'argued': 123, 'arizona.': 124, 'arm.': 125, 'army': 126, 'around': 127, 'arrive': 128, 'artist.': 129, 'as': 130, 'ask': 131, 'ask.': 132, 'ask?': 133, 'asked': 134, 'asleep.': 135, 'asleep?': 136, 'assume': 137, 'assure': 138, 'at': 139, 'ate': 140, 'attack!': 141, 'attackers': 142, 'attend': 143, 'attention.': 144, 'attitude.': 145, 'attractive.': 146, 'aunts': 147, 'australia': 148, 'australia,': 149, 'australia.': 150, 'australia?': 151, 'awards.': 152, 'aware': 153, 'away': 154, 'away.': 155, 'awful': 156, 'awkward.': 157, 'b.': 158, 'baby.': 159, 'back': 160, 'back.': 161, 'back?': 162, 'backpack.': 163, 'bad': 164, 'bad?': 165, 'badly.': 166, 'bag': 167, 'bags': 168, 'baking': 169, 'ball.': 170, 'bananas': 171, 'bananas.': 172, 'bangkok': 173, 'bank': 174, 'banker?': 175, 'bankrupt.': 176, 'barking.': 177, 'barks': 178, 'basement.': 179, 'bath': 180, 'bath.': 181, 'batteries.': 182, 'bazaar.': 183, 'be': 184, 'be.': 185, 'be?': 186, 'beans.': 187, 'bear': 188, 'beat': 189, 'beautiful': 190, 'beautiful.': 191, 'beauty': 192, 'became': 193, 'because': 194, 'become': 195, 'bed': 196, 'bed.': 197, 'bee.': 198, 'beef.': 199, 'beef?': 200, 'been': 201, 'beer,': 202, 'beer.': 203, 'beers': 204, 'beers,': 205, 'before': 206, 'before.': 207, 'began': 208, 'begin?': 209, 'beginning': 210, 'begins': 211, 'behave': 212, 'behind': 213, 'beholder.': 214, 'beijing': 215, 'being': 216, 'belgium.': 217, 'believe': 218, 'believed': 219, 'believing': 220, 'bell?': 221, 'below': 222, 'benefits': 223, 'berlin': 224, 'bern': 225, 'best': 226, 'best-selling': 227, 'best.': 228, 'best?': 229, 'bet': 230, 'better': 231, 'better.': 232, 'between': 233, 'bicycle': 234, 'bicycle.': 235, 'bicycle?': 236, 'big': 237, 'bigger.': 238, 'biggest': 239, 'billionaire,': 240, 'billionaires': 241, 'biologist.': 242, 'bird': 243, 'birthday': 244, 'birthday!': 245, 'birthday,': 246, 'birthday.': 247, 'bit': 248, 'bit.': 249, 'bitten': 250, 'bitter.': 251, 'black': 252, 'black.': 253, 'blackboard.': 254, 'blame': 255, 'blames': 256, 'blank.': 257, 'blew': 258, 'blocked': 259, 'blossoms?': 260, 'blue': 261, 'blue.': 262, 'boat.': 263, 'body.': 264, 'book': 265, 'book.': 266, 'booked.': 267, 'booklet': 268, 'books': 269, 'books.': 270, 'bored': 271, 'bored.': 272, 'born': 273, 'born.': 274, 'born?': 275, 'borrow': 276, 'borrowed': 277, 'boston': 278, 'boston.': 279, 'boston?': 280, 'both': 281, 'bother.': 282, 'bottle': 283, 'bottles': 284, 'bought': 285, 'bowline.': 286, 'box.': 287, 'boxes': 288, 'boxing': 289, 'boy.': 290, 'boyfriend': 291, 'boyfriend.': 292, 'boyfriends': 293, 'boys': 294, 'boys.': 295, 'brasilia.': 296, 'brave.': 297, 'braver': 298, 'brazil': 299, 'brazil.': 300, 'bread.': 301, 'bread?': 302, 'break': 303, 'breakfast': 304, 'breakfast.': 305, 'breathe': 306, 'bridge': 307, 'bridge.': 308, 'briefcase.': 309, 'bright': 310, 'bring': 311, 'british': 312, 'broke': 313, 'broke.': 314, 'broken.': 315, 'brother': 316, 'brother.': 317, 'brother?': 318, 'brothers.': 319, 'brought': 320, 'brown': 321, 'brussels': 322, 'bug.': 323, 'bugs.': 324, 'build': 325, 'build.': 326, 'building?': 327, 'buildings.': 328, 'bull.': 329, 'bullfighting?': 330, 'burned': 331, 'bus': 332, 'bus.': 333, 'business': 334, 'busy': 335, 'busy.': 336, 'but': 337, 'buy': 338, 'buy?': 339, 'buying': 340, 'buys': 341, 'by': 342, 'by.': 343, 'c.': 344, 'cab.': 345, 'cake': 346, 'cake,': 347, 'cake.': 348, 'cake?': 349, 'calculator.': 350, 'call': 351, 'call.': 352, 'called': 353, 'calling': 354, 'calm.': 355, 'came': 356, 'came.': 357, 'camera.': 358, 'cameras.': 359, 'can': 360, 'can.': 361, 'canada.': 362, 'canadian': 363, 'canadian.': 364, 'canals.': 365, 'canaries.': 366, 'candle.': 367, 'candy.': 368, 'cannot': 369, 'cant': 370, 'cap.': 371, 'capital': 372, 'capital.': 373, 'captain.': 374, 'car': 375, 'car.': 376, 'car?': 377, 'card.': 378, 'cards.': 379, 'care': 380, 'care,': 381, 'careful.': 382, 'carefully.': 383, 'careless?': 384, 'cares': 385, 'cars': 386, 'cars.': 387, 'cash.': 388, 'cat': 389, 'cat.': 390, 'cats': 391, 'causing': 392, 'cave.': 393, 'celebrate.': 394, 'cell': 395, 'center': 396, 'certain': 397, 'certain.': 398, 'chair': 399, 'chair.': 400, 'challenge': 401, 'chance': 402, 'chance.': 403, 'change': 404, 'change.': 405, 'changed': 406, 'changes.': 407, 'character': 408, 'charismatic': 409, 'charity.': 410, 'chased': 411, 'cheap': 412, 'cheap,': 413, 'cheap.': 414, 'cheaper.': 415, 'checked.': 416, 'cheerful': 417, 'cheese.': 418, 'chemistry': 419, 'cherry': 420, 'chess': 421, 'chess?': 422, 'chicago.': 423, 'chicago?': 424, 'chicken.': 425, 'chicken?': 426, 'children': 427, 'china.': 428, 'chinese.': 429, 'chocolate.': 430, 'choice.': 431, 'choose': 432, 'chopsticks.': 433, 'chose': 434, 'christmas': 435, 'christmas!': 436, 'christmas.': 437, 'chubby': 438, 'chubby.': 439, 'citizen,': 440, 'citizens.': 441, 'city.': 442, 'claims': 443, 'clap?': 444, 'class.': 445, 'classes,': 446, 'classmates.': 447, 'classroom': 448, 'cleaning': 449, 'clear': 450, 'clear.': 451, 'clearly.': 452, 'climate': 453, 'climb': 454, 'climbed': 455, 'clogs': 456, 'close': 457, 'close.': 458, 'close?': 459, 'closed': 460, 'closed.': 461, 'clothes': 462, 'clothes.': 463, 'clothing': 464, 'clouds.': 465, 'club.': 466, 'coach.': 467, 'coat.': 468, 'coffee': 469, 'coffee.': 470, 'coin': 471, 'coins.': 472, 'coke,': 473, 'cold': 474, 'cold.': 475, 'colder': 476, 'colleagues?': 477, 'collect': 478, 'college?': 479, 'color': 480, 'colors.': 481, 'come': 482, 'come?': 483, 'comes': 484, 'comfort.': 485, 'comfortable.': 486, 'coming': 487, 'coming.': 488, 'communication.': 489, 'companies': 490, 'company.': 491, 'compare': 492, 'competent.': 493, 'competitive.': 494, 'complain.': 495, 'complained': 496, 'complaints.': 497, 'complex.': 498, 'complicated.': 499, 'computer': 500, 'conceive': 501, 'concentrate': 502, 'concerned': 503, 'concert': 504, 'conference': 505, 'confused.': 506, 'confusing': 507, 'congratulations!': 508, 'cons.': 509, 'conservative.': 510, 'constant': 511, 'contact': 512, 'continue': 513, 'continue.': 514, 'contribute': 515, 'control.': 516, 'convalescing': 517, 'convince': 518, 'convinced': 519, 'convincing.': 520, 'cook': 521, 'cook.': 522, 'cookbooks.': 523, 'cooked?': 524, 'cookie.': 525, 'cookies.': 526, 'cooking.': 527, 'cooking?': 528, 'cool': 529, 'cooler': 530, 'cooperative.': 531, 'correct': 532, 'correct?': 533, 'cost': 534, 'cost?': 535, 'cough': 536, 'could': 537, 'couldnt': 538, 'countries': 539, 'countries.': 540, 'couple': 541, 'couple?': 542, 'courageous.': 543, 'cousins.': 544, 'covered': 545, 'cows': 546, 'crackers.': 547, 'cranky.': 548, 'crazy.': 549, 'cream.': 550, 'creative.': 551, 'cried?': 552, 'criminal.': 553, 'crowded': 554, 'cry': 555, 'cry.': 556, 'crying.': 557, 'cultures.': 558, 'cup': 559, 'cure': 560, 'curious.': 561, 'curry': 562, 'cut': 563, 'cycling.': 564, 'dad': 565, 'dad.': 566, 'daiquiri.': 567, 'damaged': 568, 'dance': 569, 'dance,': 570, 'danced': 571, 'danced.': 572, 'dangerous': 573, 'dark': 574, 'dark.': 575, 'darker': 576, 'date': 577, 'dating': 578, 'daughter.': 579, 'daughters': 580, 'day': 581, 'day!': 582, 'day.': 583, 'day?': 584, 'days': 585, 'days.': 586, 'days?': 587, 'dazed': 588, 'dead': 589, 'dead.': 590, 'deal.': 591, 'debt.': 592, 'debt?': 593, 'decide': 594, 'decide.': 595, 'decided': 596, 'decided.': 597, 'deeply.': 598, 'defeat': 599, 'defeated.': 600, 'definitely': 601, 'degree.': 602, 'delete': 603, 'delicious.': 604, 'delightful.': 605, 'depend': 606, 'derailed': 607, 'deserve': 608, 'desk.': 609, 'dessert.': 610, 'dessert?': 611, 'destroyed': 612, 'details.': 613, 'detained': 614, 'detergent.': 615, 'developed': 616, 'did': 617, 'did.': 618, 'didnt': 619, 'die': 620, 'died': 621, 'died.': 622, 'dieting.': 623, 'difference': 624, 'difficult': 625, 'difficult.': 626, 'dinner.': 627, 'director.': 628, 'disappeared': 629, 'disappointed?': 630, 'disco': 631, 'discussed': 632, 'dishes.': 633, 'disliked': 634, 'distance': 635, 'divorced.': 636, 'do': 637, 'do,': 638, 'do.': 639, 'do?': 640, 'doctor': 641, 'doctor,': 642, 'doctor.': 643, 'doctors.': 644, 'does': 645, 'does.': 646, 'doesnt': 647, 'doesnt.': 648, 'dog': 649, 'dog,': 650, 'dog.': 651, 'dogs.': 652, 'doing': 653, 'doing.': 654, 'dollars': 655, 'dollars!': 656, 'dollars.': 657, 'done': 658, 'done.': 659, 'dont': 660, 'door': 661, 'door.': 662, 'door?': 663, 'doors.': 664, 'doubt': 665, 'down': 666, 'down.': 667, 'downloaded': 668, 'downstairs': 669, 'downstairs.': 670, 'drank': 671, 'drank,': 672, 'draw': 673, 'drawer': 674, 'dream': 675, 'dream.': 676, 'dress': 677, 'dress.': 678, 'drink': 679, 'drink.': 680, 'drink?': 681, 'drinking': 682, 'drinks': 683, 'drive': 684, 'drive.': 685, 'driver': 686, 'driver.': 687, 'driving': 688, 'driving?': 689, 'drops?': 690, 'drugs.': 691, 'drummer': 692, 'drunk': 693, 'drunk.': 694, 'ducks': 695, 'dumb.': 696, 'durian.': 697, 'duties?': 698, 'dying': 699, 'dying.': 700, 'each': 701, 'earlier': 702, 'earned': 703, 'easier.': 704, 'easily.': 705, 'easy': 706, 'easy.': 707, 'eat': 708, 'eat.': 709, 'eat?': 710, 'eaten': 711, 'eating': 712, 'eating.': 713, 'eats': 714, 'egg': 715, 'eggs': 716, 'egypt.': 717, 'eight.': 718, 'eighteen': 719, 'eighteen.': 720, 'either': 721, 'elderly.': 722, 'else': 723, 'else.': 724, 'emphasize': 725, 'empty': 726, 'empty.': 727, 'encourage': 728, 'enemy': 729, 'engine.': 730, 'england.': 731, 'english': 732, 'english.': 733, 'english?': 734, 'enjoy': 735, 'enjoyed': 736, 'enjoying': 737, 'enough': 738, 'enough.': 739, 'enter': 740, 'error.': 741, 'escaped': 742, 'especially': 743, 'essay.': 744, 'even': 745, 'evening.': 746, 'evening?': 747, 'ever': 748, 'ever.': 749, 'everest': 750, 'everest.': 751, 'every': 752, 'everybody': 753, 'everybody.': 754, 'everyone': 755, 'everyone!': 756, 'everyone.': 757, 'everything': 758, 'everything.': 759, 'everything?': 760, 'everythings': 761, 'everywhere.': 762, 'exactly': 763, 'exam': 764, 'example?': 765, 'exams': 766, 'excellent.': 767, 'except': 768, 'exceptions.': 769, 'exchange': 770, 'exchanging': 771, 'excited.': 772, 'exciting.': 773, 'excuse': 774, 'exercise.': 775, 'exist.': 776, 'expect': 777, 'expect.': 778, 'expected': 779, 'expecting': 780, 'expensive': 781, 'explain': 782, 'explosion': 783, 'express': 784, 'extra': 785, 'extremely': 786, 'eye': 787, 'eyes': 788, 'eyes.': 789, 'eyesight.': 790, 'face': 791, 'face.': 792, 'facing': 793, 'fact': 794, 'facts!': 795, 'fake.': 796, 'fall.': 797, 'fall?': 798, 'fallen': 799, 'falling.': 800, 'family': 801, 'family.': 802, 'famous': 803, 'famous.': 804, 'famous?': 805, 'far': 806, 'far.': 807, 'far?': 808, 'fast': 809, 'fast.': 810, 'faster': 811, 'faster.': 812, 'father': 813, 'father.': 814, 'fathers': 815, 'fault?': 816, 'favor.': 817, 'favorite': 818, 'february': 819, 'feel': 820, 'feeling': 821, 'feeling.': 822, 'feels': 823, 'fell': 824, 'felt': 825, 'few': 826, 'fight': 827, 'fight.': 828, 'figure': 829, 'files.': 830, 'filled': 831, 'final': 832, 'finally': 833, 'financial': 834, 'find': 835, 'fine,': 836, 'finish': 837, 'finish.': 838, 'finished': 839, 'finished.': 840, 'finland.': 841, 'fire': 842, 'fire.': 843, 'first': 844, 'first.': 845, 'fish': 846, 'fish.': 847, 'fish?': 848, 'fishing.': 849, 'five': 850, 'five.': 851, 'fix': 852, 'fixed': 853, 'flashlight': 854, 'flew': 855, 'flowers': 856, 'flowers.': 857, 'flown?': 858, 'fluently?': 859, 'fly': 860, 'fog.': 861, 'folk': 862, 'follow': 863, 'food': 864, 'food.': 865, 'food?': 866, 'foolish.': 867, 'foot.': 868, 'for': 869, 'for,': 870, 'for.': 871, 'for?': 872, 'foreign': 873, 'foreigner.': 874, 'foreigners?': 875, 'forever': 876, 'forget': 877, 'forget.': 878, 'forgive': 879, 'forgot': 880, 'forgot.': 881, 'forgotten': 882, 'former': 883, 'fortune': 884, 'forty.': 885, 'forward': 886, 'found': 887, 'fountain': 888, 'four': 889, 'france.': 890, 'free': 891, 'french': 892, 'french,': 893, 'french.': 894, 'french?': 895, 'fresh': 896, 'freshman.': 897, 'fried': 898, 'friend': 899, 'friend.': 900, 'friendly': 901, 'friendly.': 902, 'friends': 903, 'friends.': 904, 'fries.': 905, 'from': 906, 'from?': 907, 'front': 908, 'fruit.': 909, 'fuji': 910, 'fuji.': 911, 'full': 912, 'full.': 913, 'fully': 914, 'fun': 915, 'fun.': 916, 'funny': 917, 'funny.': 918, 'further.': 919, 'fuse?': 920, 'future.': 921, 'future?': 922, 'game': 923, 'game?': 924, 'games.': 925, 'gas.': 926, 'gate': 927, 'gate.': 928, 'gave': 929, 'generally': 930, 'gentle': 931, 'gentleman.': 932, 'germany': 933, 'germany.': 934, 'get': 935, 'get,': 936, 'gets': 937, 'getting': 938, 'ghosts': 939, 'gift.': 940, 'gingerbread.': 941, 'girl': 942, 'girl.': 943, 'girlfriend': 944, 'girlfriend.': 945, 'girls': 946, 'give': 947, 'glad': 948, 'glasses.': 949, 'go': 950, 'go,': 951, 'go.': 952, 'go?': 953, 'goal.': 954, 'godmother.': 955, 'goes': 956, 'going': 957, 'going?': 958, 'gold': 959, 'golf?': 960, 'gone': 961, 'gone.': 962, 'gone?': 963, 'good': 964, 'good-looking.': 965, 'good.': 966, 'goodbye.': 967, 'got': 968, 'gotten': 969, 'grabbed': 970, 'grade.': 971, 'gradually,': 972, 'graduated': 973, 'grammar': 974, 'grandfather': 975, 'grandmother': 976, 'grass.': 977, 'grateful': 978, 'great': 979, 'great-grandmother.': 980, 'greatest': 981, 'grew': 982, 'group': 983, 'grow': 984, 'grows': 985, 'guess': 986, 'guess.': 987, 'guide.': 988, 'guilty': 989, 'guitar.': 990, 'gun.': 991, 'guy,': 992, 'guy.': 993, 'guys': 994, 'habits?': 995, 'had': 996, 'hadnt': 997, 'hair.': 998, 'haircut,': 999, 'half': 1000, 'half-brother.': 1001, 'halloween': 1002, 'hamburgers': 1003, 'hammer?': 1004, 'hand': 1005, 'hand.': 1006, 'handle': 1007, 'hands': 1008, 'hands.': 1009, 'happen': 1010, 'happen,': 1011, 'happen?': 1012, 'happened': 1013, 'happened.': 1014, 'happening': 1015, 'happens.': 1016, 'happy': 1017, 'happy.': 1018, 'hard': 1019, 'hard.': 1020, 'harvard.': 1021, 'has': 1022, 'hasnt': 1023, 'hat': 1024, 'hat,': 1025, 'hat.': 1026, 'hate': 1027, 'hated': 1028, 'hates': 1029, 'haunted.': 1030, 'have': 1031, 'have.': 1032, 'havent': 1033, 'having': 1034, 'hayashi': 1035, 'he': 1036, 'he?': 1037, 'head': 1038, 'headache': 1039, 'headache.': 1040, 'headed': 1041, 'headline': 1042, 'health': 1043, 'healthy': 1044, 'hear': 1045, 'heard': 1046, 'heart.': 1047, 'heat.': 1048, 'heaven.': 1049, 'heavier,': 1050, 'heavy.': 1051, 'hed': 1052, 'heights.': 1053, 'hello!': 1054, 'help': 1055, 'help.': 1056, 'help?': 1057, 'helped': 1058, 'helpful.': 1059, 'helping': 1060, 'helsinki': 1061, 'her': 1062, 'her.': 1063, 'here': 1064, 'here,': 1065, 'here.': 1066, 'here?': 1067, 'herself?': 1068, 'hes': 1069, 'hi,': 1070, 'hiccups.': 1071, 'hidden': 1072, 'hide.': 1073, 'hiding': 1074, 'high': 1075, 'highest': 1076, 'highly': 1077, 'hiking.': 1078, 'him': 1079, 'him.': 1080, 'himself': 1081, 'himself.': 1082, 'hippopotamus': 1083, 'hire': 1084, 'his': 1085, 'history': 1086, 'history!': 1087, 'hit': 1088, 'hobby.': 1089, 'hold': 1090, 'holding': 1091, 'holiday': 1092, 'home': 1093, 'home.': 1094, 'homework': 1095, 'homework.': 1096, 'honestly.': 1097, 'hope': 1098, 'hoping': 1099, 'horizon.': 1100, 'hospital': 1101, 'hospital,': 1102, 'hospital.': 1103, 'hot': 1104, 'hotel.': 1105, 'hour.': 1106, 'hours': 1107, 'hours.': 1108, 'house': 1109, 'house.': 1110, 'house?': 1111, 'houses': 1112, 'how': 1113, 'hows': 1114, 'human.': 1115, 'humiliating.': 1116, 'hunger.': 1117, 'hungry,': 1118, 'hungry.': 1119, 'hunting.': 1120, 'hurry': 1121, 'hurry.': 1122, 'hurt': 1123, 'hurt.': 1124, 'hurts.': 1125, 'husband.': 1126, 'husbands': 1127, 'i': 1128, 'i?': 1129, 'ice': 1130, 'id': 1131, 'idea': 1132, 'idea.': 1133, 'ideas': 1134, 'ideas.': 1135, 'identify': 1136, 'if': 1137, 'ignore': 1138, 'ill': 1139, 'ill.': 1140, 'im': 1141, 'im,': 1142, 'imagining': 1143, 'immediately.': 1144, 'import': 1145, 'important': 1146, 'important.': 1147, 'impossible.': 1148, 'impress': 1149, 'impressed.': 1150, 'impressive.': 1151, 'improve': 1152, 'in': 1153, 'in.': 1154, 'in?': 1155, 'incentive.': 1156, 'including': 1157, 'india.': 1158, 'inevitable.': 1159, 'influential.': 1160, 'injured.': 1161, 'insects.': 1162, 'insecure': 1163, 'inside.': 1164, 'instead.': 1165, 'intelligent': 1166, 'intends': 1167, 'interested': 1168, 'interested.': 1169, 'interesting': 1170, 'internet.': 1171, 'intersection.': 1172, 'into': 1173, 'involved': 1174, 'involved.': 1175, 'irish': 1176, 'iron': 1177, 'is': 1178, 'is.': 1179, 'is?': 1180, 'island.': 1181, 'isnt': 1182, 'isnt.': 1183, 'issues?': 1184, 'it': 1185, 'it,': 1186, 'it.': 1187, 'it?': 1188, 'italy': 1189, 'itll': 1190, 'its': 1191, 'ive': 1192, 'jackson': 1193, 'jackson.': 1194, 'jackson?': 1195, 'jail.': 1196, 'japan': 1197, 'japan.': 1198, 'japanese': 1199, 'japanese.': 1200, 'japanese?': 1201, 'jazz.': 1202, 'jealous,': 1203, 'jeans.': 1204, 'job': 1205, 'job.': 1206, 'jobs': 1207, 'john.': 1208, 'join': 1209, 'joined': 1210, 'joke': 1211, 'joke,': 1212, 'judged': 1213, 'juice.': 1214, 'july.': 1215, 'jumped': 1216, 'just': 1217, 'keep': 1218, 'kept': 1219, 'ketchup': 1220, 'key.': 1221, 'keys': 1222, 'kid': 1223, 'kid.': 1224, 'kidding!': 1225, 'kids': 1226, 'kids,': 1227, 'killed': 1228, 'killed.': 1229, 'kilometers': 1230, 'kind': 1231, 'kind.': 1232, 'king': 1233, 'kiss': 1234, 'kissed': 1235, 'kite.': 1236, 'kitten.': 1237, 'knew': 1238, 'knife': 1239, 'knocked': 1240, 'know': 1241, 'know,': 1242, 'know.': 1243, 'knowing': 1244, 'known': 1245, 'knows': 1246, 'knows.': 1247, 'korean': 1248, 'kosher': 1249, 'kyiv': 1250, 'kyiv.': 1251, 'lamp': 1252, 'language': 1253, 'language.': 1254, 'languages': 1255, 'languages,': 1256, 'languages.': 1257, 'laptop': 1258, 'laptop,': 1259, 'laptop?': 1260, 'large': 1261, 'large.': 1262, 'larger': 1263, 'larger.': 1264, 'last': 1265, 'late': 1266, 'late.': 1267, 'lately?': 1268, 'later?': 1269, 'latin.': 1270, 'laugh.': 1271, 'laughing.': 1272, 'laughs': 1273, 'laughter.': 1274, 'laundry': 1275, 'law': 1276, 'lazy.': 1277, 'lead': 1278, 'learn': 1279, 'learn.': 1280, 'learned': 1281, 'learning': 1282, 'least': 1283, 'leather.': 1284, 'leave': 1285, 'leave.': 1286, 'leaves': 1287, 'leaving': 1288, 'left': 1289, 'left.': 1290, 'legs.': 1291, 'lend': 1292, 'lent': 1293, 'let': 1294, 'lets': 1295, 'letter': 1296, 'letter.': 1297, 'letter?': 1298, 'letters.': 1299, 'library': 1300, 'lie.': 1301, 'lied': 1302, 'life': 1303, 'lifetime.': 1304, 'light': 1305, 'light.': 1306, 'lightning': 1307, 'like': 1308, 'like.': 1309, 'like?': 1310, 'liked': 1311, 'likely': 1312, 'likes': 1313, 'limping': 1314, 'line': 1315, 'lion.': 1316, 'lips.': 1317, 'liquid': 1318, 'list': 1319, 'list.': 1320, 'listed.': 1321, 'listen.': 1322, 'listened': 1323, 'listening': 1324, 'listening.': 1325, 'lit': 1326, 'liter': 1327, 'literally': 1328, 'little': 1329, 'little.': 1330, 'little?': 1331, 'live': 1332, 'live.': 1333, 'lived': 1334, 'lived.': 1335, 'lives': 1336, 'lives.': 1337, 'living': 1338, 'loan.': 1339, 'local?': 1340, 'locked': 1341, 'log': 1342, 'london': 1343, 'lonely.': 1344, 'long': 1345, 'long.': 1346, 'long?': 1347, 'longer': 1348, 'longer.': 1349, 'longer?': 1350, 'look': 1351, 'look.': 1352, 'looked': 1353, 'looking': 1354, 'looks': 1355, 'lose': 1356, 'lose.': 1357, 'loss': 1358, 'lost': 1359, 'lost.': 1360, 'lot': 1361, 'lot.': 1362, 'louder.': 1363, 'love': 1364, 'love.': 1365, 'loved': 1366, 'lovely!': 1367, 'lover.': 1368, 'loves': 1369, 'low.': 1370, 'lower': 1371, 'lucky': 1372, 'lucky.': 1373, 'lunch': 1374, 'lunch.': 1375, 'lunch?': 1376, 'lying': 1377, 'lying.': 1378, 'machine.': 1379, 'mad.': 1380, 'made': 1381, 'made.': 1382, 'made?': 1383, 'madrid': 1384, 'mail': 1385, 'mailbox': 1386, 'make': 1387, 'makes': 1388, 'making': 1389, 'man': 1390, 'man.': 1391, 'man?': 1392, 'many': 1393, 'map': 1394, 'map.': 1395, 'marine': 1396, 'mark': 1397, 'married': 1398, 'married.': 1399, 'marry': 1400, 'marshmallows?': 1401, 'mary': 1402, 'mary.': 1403, 'mary?': 1404, 'marys': 1405, 'marys.': 1406, 'mask.': 1407, 'material.': 1408, 'math': 1409, 'mathematics': 1410, 'mathematics.': 1411, 'matter': 1412, 'matter.': 1413, 'may': 1414, 'maybe': 1415, 'me': 1416, 'me,': 1417, 'me.': 1418, 'me?': 1419, 'mean': 1420, 'meaning': 1421, 'means': 1422, 'meat.': 1423, 'meat?': 1424, 'medicine.': 1425, 'meet': 1426, 'meeting.': 1427, 'meeting?': 1428, 'meetings.': 1429, 'member.': 1430, 'men': 1431, 'men.': 1432, 'merry': 1433, 'message.': 1434, 'met': 1435, 'met.': 1436, 'mexican': 1437, 'microwave': 1438, 'middle': 1439, 'might': 1440, 'mightve': 1441, 'miles': 1442, 'milk': 1443, 'milk.': 1444, 'million': 1445, 'millionaire.': 1446, 'mind': 1447, 'mine.': 1448, 'minute.': 1449, 'minutes': 1450, 'minutes.': 1451, 'misleading.': 1452, 'miss': 1453, 'mistake.': 1454, 'mistaken.': 1455, 'mistakes.': 1456, 'model': 1457, 'model.': 1458, 'modest': 1459, 'moment.': 1460, 'monday': 1461, 'monday.': 1462, 'mondays?': 1463, 'money': 1464, 'money.': 1465, 'money?': 1466, 'mongolia.': 1467, 'month': 1468, 'month.': 1469, 'months': 1470, 'mood.': 1471, 'mood?': 1472, 'moon': 1473, 'more': 1474, 'more.': 1475, 'more?': 1476, 'morning': 1477, 'morning.': 1478, 'mosquitoes': 1479, 'most': 1480, 'mother': 1481, 'mother.': 1482, 'motor': 1483, 'motorcycle?': 1484, 'mount': 1485, 'mountain': 1486, 'mountaintops': 1487, 'mouth': 1488, 'moved': 1489, 'movie.': 1490, 'movie?': 1491, 'movies': 1492, 'movies.': 1493, 'mt.': 1494, 'much': 1495, 'much,': 1496, 'much.': 1497, 'mug': 1498, 'mushrooms': 1499, 'music.': 1500, 'musician.': 1501, 'must': 1502, 'my': 1503, 'myself': 1504, 'myself.': 1505, 'naka-meguro.': 1506, 'name': 1507, 'name.': 1508, 'name?': 1509, 'names': 1510, 'names.': 1511, 'names?': 1512, 'narrow.': 1513, 'naughty': 1514, 'near': 1515, 'nearby.': 1516, 'nearly': 1517, 'necessary.': 1518, 'need': 1519, 'need?': 1520, 'needed': 1521, 'needs': 1522, 'neighbor?': 1523, 'neighborhood': 1524, 'neither': 1525, 'nervous': 1526, 'nervous.': 1527, 'netherlands': 1528, 'never': 1529, 'new': 1530, 'new.': 1531, 'next': 1532, 'next.': 1533, 'next?': 1534, 'nice': 1535, 'nice.': 1536, 'nickname.': 1537, 'night': 1538, 'night,': 1539, 'night.': 1540, 'night?': 1541, 'nine.': 1542, 'no': 1543, 'nobel': 1544, 'nobody': 1545, 'noisy': 1546, 'nor': 1547, 'normal': 1548, 'not': 1549, 'not.': 1550, 'notebook?': 1551, 'notes.': 1552, 'nothing': 1553, 'notice': 1554, 'notice.': 1555, 'novels.': 1556, 'now': 1557, 'now,': 1558, 'now.': 1559, 'now?': 1560, 'nowadays': 1561, 'nowadays.': 1562, 'number': 1563, 'number.': 1564, 'numbers': 1565, 'nursing': 1566, 'nutritious.': 1567, 'obnoxious.': 1568, 'obvious.': 1569, 'obviously': 1570, 'obviously,': 1571, 'ocean': 1572, 'oclock.': 1573, 'october': 1574, 'october.': 1575, 'of': 1576, 'off': 1577, 'off.': 1578, 'off?': 1579, 'offended.': 1580, 'offered': 1581, 'office': 1582, 'office.': 1583, 'office?': 1584, 'often': 1585, 'often?': 1586, 'oh,': 1587, 'ok': 1588, 'ok,': 1589, 'ok.': 1590, 'ok?': 1591, 'okay.': 1592, 'old': 1593, 'old.': 1594, 'older': 1595, 'on': 1596, 'on.': 1597, 'on?': 1598, 'once': 1599, 'once.': 1600, 'one': 1601, 'one.': 1602, 'one?': 1603, 'ones.': 1604, 'only': 1605, 'open': 1606, 'open.': 1607, 'opened': 1608, 'opens': 1609, 'opinion': 1610, 'opinion.': 1611, 'or': 1612, 'orange.': 1613, 'oranges': 1614, 'ostriches': 1615, 'other': 1616, 'other.': 1617, 'otherwise.': 1618, 'ottawa': 1619, 'ought': 1620, 'our': 1621, 'out': 1622, 'out.': 1623, 'outside': 1624, 'outside.': 1625, 'oven.': 1626, 'over': 1627, 'over.': 1628, 'overreact.': 1629, 'overseas': 1630, 'overseas.': 1631, 'overweight.': 1632, 'own': 1633, 'pack': 1634, 'page': 1635, 'paid': 1636, 'paid.': 1637, 'pain.': 1638, 'pain?': 1639, 'painful': 1640, 'paint?': 1641, 'painted': 1642, 'palace.': 1643, 'pancakes.': 1644, 'papers': 1645, 'parents': 1646, 'parents.': 1647, 'paris': 1648, 'park': 1649, 'park.': 1650, 'parked': 1651, 'parking': 1652, 'part': 1653, 'particularly': 1654, 'party.': 1655, 'passed': 1656, 'passengers': 1657, 'passport,': 1658, 'pattern.': 1659, 'pay': 1660, 'paying': 1661, 'payment': 1662, 'peach': 1663, 'peak': 1664, 'peas.': 1665, 'pen': 1666, 'pencil.': 1667, 'pens.': 1668, 'people': 1669, 'people,': 1670, 'people.': 1671, 'per': 1672, 'perfect': 1673, 'person': 1674, 'person.': 1675, 'pet.': 1676, 'phoenix': 1677, 'phone': 1678, 'phone.': 1679, 'phone?': 1680, 'photographs': 1681, 'pick': 1682, 'picture': 1683, 'picture.': 1684, 'picture?': 1685, 'piece': 1686, 'pig': 1687, 'pig.': 1688, 'pizza': 1689, 'pizza?': 1690, 'place': 1691, 'place,': 1692, 'place.': 1693, 'plan': 1694, 'plan.': 1695, 'plane': 1696, 'planes': 1697, 'planes.': 1698, 'planning': 1699, 'plans': 1700, 'plans.': 1701, 'plans?': 1702, 'planted': 1703, 'plants': 1704, 'plants?': 1705, 'play': 1706, 'played': 1707, 'player': 1708, 'player.': 1709, 'playing': 1710, 'plays': 1711, 'please': 1712, 'please.': 1713, 'please?': 1714, 'pleased': 1715, 'pleased.': 1716, 'plumber.': 1717, 'pocket.': 1718, 'pocketknife.': 1719, 'poem': 1720, 'poems.': 1721, 'point': 1722, 'point.': 1723, 'pointing': 1724, 'poland': 1725, 'police': 1726, 'police.': 1727, 'policeman': 1728, 'polite.': 1729, 'pond.': 1730, 'poor': 1731, 'poor.': 1732, 'pop': 1733, 'popular': 1734, 'pork,': 1735, 'pork.': 1736, 'positive': 1737, 'possible,': 1738, 'potential.': 1739, 'power.': 1740, 'powerful.': 1741, 'practical.': 1742, 'pray': 1743, 'predict': 1744, 'predictable.': 1745, 'prefer': 1746, 'preparations': 1747, 'pressure': 1748, 'pretended': 1749, 'pretty': 1750, 'pretty.': 1751, 'price': 1752, 'price?': 1753, 'prices?': 1754, 'prime': 1755, 'prison': 1756, 'prize,': 1757, 'proactive.': 1758, 'probably': 1759, 'problem': 1760, 'problem.': 1761, 'problem?': 1762, 'problems.': 1763, 'process': 1764, 'produces': 1765, 'products': 1766, 'professor.': 1767, 'promised': 1768, 'pronouncing': 1769, 'proof': 1770, 'proof.': 1771, 'pros': 1772, 'protect': 1773, 'proved': 1774, 'public': 1775, 'public.': 1776, 'pulled': 1777, 'punch': 1778, 'puppy': 1779, 'purpose.': 1780, 'put': 1781, 'question': 1782, 'question.': 1783, 'question?': 1784, 'questions': 1785, 'questions.': 1786, 'quicker': 1787, 'quickly': 1788, 'quickly.': 1789, 'quiet': 1790, 'quiet.': 1791, 'quietly.': 1792, 'quit': 1793, 'quit.': 1794, 'quit?': 1795, 'quite': 1796, 'radio': 1797, 'radio.': 1798, 'radio?': 1799, 'rain': 1800, 'rain.': 1801, 'rained': 1802, 'raining': 1803, 'raining.': 1804, 'raise': 1805, 'ramen.': 1806, 'ran': 1807, 'rang': 1808, 'rare.': 1809, 'rate?': 1810, 'rather': 1811, 'reach': 1812, 'read': 1813, 'read.': 1814, 'read?': 1815, 'reading': 1816, 'reading.': 1817, 'ready': 1818, 'ready.': 1819, 'real': 1820, 'realize': 1821, 'realized': 1822, 'really': 1823, 'really?': 1824, 'received': 1825, 'recent': 1826, 'red': 1827, 'red.': 1828, 'refrigerator.': 1829, 'refused': 1830, 'regrets.': 1831, 'relationship': 1832, 'relatives.': 1833, 'relax.': 1834, 'reluctant': 1835, 'remained': 1836, 'remember': 1837, 'remembers': 1838, 'remind': 1839, 'rent': 1840, 'rent.': 1841, 'repair': 1842, 'repeat': 1843, 'repeated': 1844, 'rephrase': 1845, 'replaceable.': 1846, 'reply.': 1847, 'report': 1848, 'report.': 1849, 'report?': 1850, 'reprimanded': 1851, 'requested,': 1852, 'required': 1853, 'respect': 1854, 'respect.': 1855, 'response': 1856, 'rest': 1857, 'restaurant': 1858, 'restaurant.': 1859, 'restroom?': 1860, 'results': 1861, 'return': 1862, 'rewarded.': 1863, 'rice': 1864, 'rice,': 1865, 'rice.': 1866, 'rich': 1867, 'rich.': 1868, 'ride': 1869, 'riding.': 1870, 'right': 1871, 'right.': 1872, 'right?': 1873, 'ring': 1874, 'rings': 1875, 'risk.': 1876, 'risky': 1877, 'risky.': 1878, 'rival.': 1879, 'river.': 1880, 'robbed': 1881, 'rock': 1882, 'rolled': 1883, 'rome.': 1884, 'roof': 1885, 'room': 1886, 'room.': 1887, 'roommate.': 1888, 'rooms': 1889, 'rosemary.': 1890, 'roses': 1891, 'roses.': 1892, 'round': 1893, 'rude': 1894, 'rude.': 1895, 'rule,': 1896, 'rule.': 1897, 'rumored': 1898, 'run': 1899, 'run.': 1900, 'run?': 1901, 'runner.': 1902, 'running.': 1903, 'sacred': 1904, 'sad.': 1905, 'safe': 1906, 'safe.': 1907, 'said': 1908, 'said.': 1909, 'sale': 1910, 'same': 1911, 'same.': 1912, 'sandwiches': 1913, 'sandwiches.': 1914, 'sang': 1915, 'santas': 1916, 'sashimi.': 1917, 'sat': 1918, 'save': 1919, 'saw': 1920, 'say': 1921, 'say.': 1922, 'say?': 1923, 'saying.': 1924, 'says': 1925, 'says.': 1926, 'scare': 1927, 'scared': 1928, 'scary.': 1929, 'school': 1930, 'school,': 1931, 'school.': 1932, 'school?': 1933, 'schoolbooks.': 1934, 'scored': 1935, 'scripts.': 1936, 'sea': 1937, 'searched': 1938, 'searching': 1939, 'seat': 1940, 'secret.': 1941, 'secret?': 1942, 'see': 1943, 'seem': 1944, 'seemed': 1945, 'seems': 1946, 'seen': 1947, 'seen.': 1948, 'send': 1949, 'senior': 1950, 'seniority.': 1951, 'sense.': 1952, 'sent': 1953, 'sentence?': 1954, 'serious': 1955, 'serious.': 1956, 'seriously': 1957, 'served': 1958, 'set': 1959, 'setting': 1960, 'seven': 1961, 'several': 1962, 'sewing': 1963, 'shake': 1964, 'shall': 1965, 'shallow,': 1966, 'shape': 1967, 'shape.': 1968, 'share.': 1969, 'sharkskin': 1970, 'sharp.': 1971, 'she': 1972, 'she?': 1973, 'sheep': 1974, 'shibuya': 1975, 'shirt.': 1976, 'shirts': 1977, 'shirtsleeves': 1978, 'shirtsleeves.': 1979, 'shock.': 1980, 'shocked.': 1981, 'shoe': 1982, 'shoes.': 1983, 'shoot': 1984, 'shop': 1985, 'shop.': 1986, 'shopping.': 1987, 'short': 1988, 'short.': 1989, 'shortest': 1990, 'shot.': 1991, 'should': 1992, 'shouldnt': 1993, 'shouldve': 1994, 'shouldve.': 1995, 'show': 1996, 'showed': 1997, 'showing': 1998, 'sick': 1999, 'sick.': 2000, 'side': 2001, 'side.': 2002, 'sightseeing.': 2003, 'sign': 2004, 'silver': 2005, 'simple': 2006, 'simple.': 2007, 'since': 2008, 'sing?': 2009, 'singer': 2010, 'singer?': 2011, 'singing?': 2012, 'single': 2013, 'sir?': 2014, 'sister': 2015, 'sister?': 2016, 'sisters': 2017, 'sisters?': 2018, 'sit': 2019, 'sitting': 2020, 'situation': 2021, 'six-thirty.': 2022, 'sixteen': 2023, 'size': 2024, 'size.': 2025, 'skiing.': 2026, 'skin': 2027, 'skip': 2028, 'skipper.': 2029, 'sleep': 2030, 'sleep.': 2031, 'sleeping': 2032, 'sleeping.': 2033, 'sleeping?': 2034, 'sleepy,': 2035, 'sleepy.': 2036, 'sleepy?': 2037, 'slept': 2038, 'slight': 2039, 'slow.': 2040, 'slowly': 2041, 'slowly.': 2042, 'small': 2043, 'small.': 2044, 'smart': 2045, 'smart.': 2046, 'smartest': 2047, 'smell': 2048, 'smile.': 2049, 'smiling.': 2050, 'smoke.': 2051, 'snake.': 2052, 'snow': 2053, 'snowing': 2054, 'so': 2055, 'so.': 2056, 'soccer': 2057, 'socks.': 2058, 'sold': 2059, 'solve': 2060, 'some': 2061, 'some.': 2062, 'someone': 2063, 'something': 2064, 'something.': 2065, 'sometimes': 2066, 'somewhat': 2067, 'somewhere': 2068, 'somewhere,': 2069, 'son': 2070, 'son-in-law.': 2071, 'son.': 2072, 'song': 2073, 'song,': 2074, 'song.': 2075, 'soon': 2076, 'soon.': 2077, 'sorry': 2078, 'sorry,': 2079, 'sorry.': 2080, 'sound': 2081, 'sour.': 2082, 'source': 2083, 'spain.': 2084, 'speak': 2085, 'speak.': 2086, 'speak?': 2087, 'speaking': 2088, 'speaking.': 2089, 'speaking?': 2090, 'speaks': 2091, 'special': 2092, 'speeches': 2093, 'speed': 2094, 'spell': 2095, 'spend': 2096, 'spends': 2097, 'spent': 2098, 'spider.': 2099, 'spilled': 2100, 'spoken': 2101, 'sports.': 2102, 'spots': 2103, 'sprang': 2104, 'spread': 2105, 'staff': 2106, 'standing': 2107, 'staring': 2108, 'start?': 2109, 'started': 2110, 'started.': 2111, 'starved.': 2112, 'starving.': 2113, 'states.': 2114, 'station': 2115, 'station?': 2116, 'stay': 2117, 'stay.': 2118, 'stay?': 2119, 'stayed': 2120, 'staying': 2121, 'steak.': 2122, 'steal': 2123, 'still': 2124, 'stole': 2125, 'stolen': 2126, 'stop': 2127, 'stop.': 2128, 'stopped': 2129, 'stopping?': 2130, 'store': 2131, 'store.': 2132, 'story': 2133, 'story.': 2134, 'stove.': 2135, 'straight': 2136, 'strange': 2137, 'stranger?': 2138, 'strapless': 2139, 'street': 2140, 'street.': 2141, 'street?': 2142, 'stretch': 2143, 'stretch.': 2144, 'strict,': 2145, 'strong.': 2146, 'strong?': 2147, 'stronger,': 2148, 'student': 2149, 'student.': 2150, 'students': 2151, 'students.': 2152, 'study': 2153, 'studying': 2154, 'studying.': 2155, 'studying?': 2156, 'stuff': 2157, 'stupid.': 2158, 'subject.': 2159, 'submitted': 2160, 'submitting': 2161, 'suburbs': 2162, 'succeed.': 2163, 'success.': 2164, 'such': 2165, 'sugar': 2166, 'sugar.': 2167, 'suggestion.': 2168, 'suggestions.': 2169, 'suitcase': 2170, 'suitcases?': 2171, 'suits': 2172, 'summary': 2173, 'summer': 2174, 'summer.': 2175, 'sun': 2176, 'sunday.': 2177, 'sunday?': 2178, 'sunglasses': 2179, 'sunglasses.': 2180, 'sunny': 2181, 'sunscreen.': 2182, 'supermarket': 2183, 'supper': 2184, 'supplies': 2185, 'supply.': 2186, 'suppose': 2187, 'supposed': 2188, 'sure': 2189, 'sure.': 2190, 'surgery.': 2191, 'surprised': 2192, 'survive.': 2193, 'suspect': 2194, 'suspended.': 2195, 'suspension': 2196, 'swear': 2197, 'sweater.': 2198, 'sweet.': 2199, 'swim': 2200, 'swim.': 2201, 'swimming': 2202, 'swimming.': 2203, 'switzerland.': 2204, 'system.': 2205, 'table.': 2206, 'tag.': 2207, 'tail': 2208, 'tails.': 2209, 'take': 2210, 'take.': 2211, 'taken': 2212, 'taken?': 2213, 'taking': 2214, 'talented': 2215, 'talented,': 2216, 'talented.': 2217, 'talented?': 2218, 'talk': 2219, 'talk.': 2220, 'talkative.': 2221, 'talking': 2222, 'talking.': 2223, 'tall': 2224, 'tall.': 2225, 'taller,': 2226, 'taller.': 2227, 'tallest': 2228, 'tattoo?': 2229, 'taught': 2230, 'tax.': 2231, 'taxi': 2232, 'tea': 2233, 'tea,': 2234, 'tea.': 2235, 'teach': 2236, 'teacher': 2237, 'teacher.': 2238, 'teacher?': 2239, 'teachers.': 2240, 'teaches': 2241, 'teaching': 2242, 'team': 2243, 'team.': 2244, 'teammate.': 2245, 'teams': 2246, 'teenager.': 2247, 'television.': 2248, 'televisions': 2249, 'tell': 2250, 'telling': 2251, 'tells': 2252, 'temporary.': 2253, 'ten': 2254, 'tends': 2255, 'tennis': 2256, 'terrible': 2257, 'terribly': 2258, 'terrified.': 2259, 'tests.': 2260, 'texas.': 2261, 'th.': 2262, 'thailand.': 2263, 'than': 2264, 'thank': 2265, 'thanks': 2266, 'that': 2267, 'that,': 2268, 'that.': 2269, 'that?': 2270, 'thatll': 2271, 'thats': 2272, 'the': 2273, 'theater.': 2274, 'theft.': 2275, 'them': 2276, 'them.': 2277, 'then': 2278, 'then.': 2279, 'there': 2280, 'there,': 2281, 'there.': 2282, 'there?': 2283, 'theres': 2284, 'these': 2285, 'these.': 2286, 'they': 2287, 'they?': 2288, 'theyll': 2289, 'theyre': 2290, 'theyve': 2291, 'thick': 2292, 'thing': 2293, 'thing.': 2294, 'thing?': 2295, 'things': 2296, 'things.': 2297, 'things?': 2298, 'think': 2299, 'think,': 2300, 'think.': 2301, 'think?': 2302, 'thinker.': 2303, 'thinking': 2304, 'thinks': 2305, 'third': 2306, 'thirsty.': 2307, 'thirteen': 2308, 'thirteen.': 2309, 'thirty': 2310, 'thirty.': 2311, 'this': 2312, 'this.': 2313, 'this?': 2314, 'thisll': 2315, 'those': 2316, 'those.': 2317, 'thought': 2318, 'thought.': 2319, 'thousand': 2320, 'threatened': 2321, 'threatened.': 2322, 'three': 2323, 'three-year-old': 2324, 'threw': 2325, 'throw': 2326, 'ticket.': 2327, 'ticket?': 2328, 'tickets': 2329, 'tie': 2330, 'tie.': 2331, 'tight-fitting': 2332, 'tiki.': 2333, 'time': 2334, 'time.': 2335, 'time?': 2336, 'times': 2337, 'tired': 2338, 'tired.': 2339, 'title,': 2340, 'to': 2341, 'to,': 2342, 'to.': 2343, 'to?': 2344, 'today': 2345, 'today,': 2346, 'today.': 2347, 'today?': 2348, 'todays': 2349, 'toe': 2350, 'together': 2351, 'together.': 2352, 'together?': 2353, 'toilet?': 2354, 'tokyo': 2355, 'tokyo.': 2356, 'told': 2357, 'tom': 2358, 'tom,': 2359, 'tom.': 2360, 'tom?': 2361, 'tomatoes.': 2362, 'tomorrow': 2363, 'tomorrow.': 2364, 'tomorrow?': 2365, 'tomorrows': 2366, 'toms': 2367, 'toms.': 2368, 'toms?': 2369, 'tonight.': 2370, 'tonight?': 2371, 'too': 2372, 'too.': 2373, 'too?': 2374, 'took': 2375, 'toolbox': 2376, 'top': 2377, 'tossed': 2378, 'totally': 2379, 'touch': 2380, 'touching?': 2381, 'tour': 2382, 'toward': 2383, 'town': 2384, 'town.': 2385, 'townie?': 2386, 'toy': 2387, 'toys.': 2388, 'train': 2389, 'train.': 2390, 'trainer.': 2391, 'translate.': 2392, 'translated': 2393, 'translator.': 2394, 'travel': 2395, 'tree.': 2396, 'tried': 2397, 'trip': 2398, 'trip?': 2399, 'trouble': 2400, 'trouble.': 2401, 'true.': 2402, 'truly': 2403, 'trunk.': 2404, 'trust': 2405, 'trust.': 2406, 'truth': 2407, 'truthful?': 2408, 'try': 2409, 'try.': 2410, 'trying': 2411, 'tuesday.': 2412, 'tulips,': 2413, 'tune.': 2414, 'turn': 2415, 'turned': 2416, 'tv': 2417, 'tv.': 2418, 'twelve.': 2419, 'twice': 2420, 'twice.': 2421, 'two': 2422, 'two.': 2423, 'ugly': 2424, 'ukraine': 2425, 'ukraine.': 2426, 'ulaanbaatar': 2427, 'umbrella': 2428, 'umbrella,': 2429, 'umbrella.': 2430, 'umbrellas.': 2431, 'uncomfortable.': 2432, 'unconscious.': 2433, 'unconstitutional?': 2434, 'under': 2435, 'underestimate': 2436, 'understand': 2437, 'understand.': 2438, 'understands': 2439, 'understands?': 2440, 'understood': 2441, 'unemployed.': 2442, 'unethical': 2443, 'unhappy,': 2444, 'united': 2445, 'unlocked.': 2446, 'unlucky.': 2447, 'unmarried.': 2448, 'unprofessional.': 2449, 'unsociable.': 2450, 'until': 2451, 'up': 2452, 'up,': 2453, 'up.': 2454, 'upset': 2455, 'upset.': 2456, 'upstairs.': 2457, 'urged': 2458, 'uruguay.': 2459, 'us': 2460, 'us,': 2461, 'us.': 2462, 'use': 2463, 'use?': 2464, 'used': 2465, 'usually': 2466, 'vacation?': 2467, 'van.': 2468, 'vegetables.': 2469, 'very': 2470, 'video': 2471, 'video.': 2472, 'view.': 2473, 'vinegar.': 2474, 'violence': 2475, 'violent.': 2476, 'violin': 2477, 'violin.': 2478, 'visit': 2479, 'visit.': 2480, 'visited': 2481, 'vitamin': 2482, 'volunteer.': 2483, 'vomiting.': 2484, 'wait': 2485, 'wait.': 2486, 'wait?': 2487, 'waited': 2488, 'waiter': 2489, 'waiting': 2490, 'waking': 2491, 'walk': 2492, 'walk.': 2493, 'walked': 2494, 'walking': 2495, 'wall.': 2496, 'wallet': 2497, 'waned': 2498, 'want': 2499, 'want.': 2500, 'wanted': 2501, 'wanted.': 2502, 'wants': 2503, 'wants.': 2504, 'warsaw.': 2505, 'was': 2506, 'was.': 2507, 'wash': 2508, 'washed': 2509, 'washing': 2510, 'washing?': 2511, 'wasnt': 2512, 'waste': 2513, 'watch': 2514, 'watch.': 2515, 'watched': 2516, 'watches': 2517, 'watching': 2518, 'water': 2519, 'water.': 2520, 'water?': 2521, 'waxed': 2522, 'way': 2523, 'we': 2524, 'wealthy.': 2525, 'wear': 2526, 'wearing': 2527, 'wearing?': 2528, 'wears': 2529, 'wears.': 2530, 'weather': 2531, 'weather.': 2532, 'weathers': 2533, 'website': 2534, 'wed': 2535, 'week': 2536, 'week.': 2537, 'weeks': 2538, 'weighed': 2539, 'weight.': 2540, 'welcome': 2541, 'well': 2542, 'well,': 2543, 'well.': 2544, 'went': 2545, 'went,': 2546, 'went.': 2547, 'were': 2548, 'werent': 2549, 'weve': 2550, 'whale': 2551, 'what': 2552, 'whatever': 2553, 'whatll': 2554, 'whats': 2555, 'wheel?': 2556, 'wheels.': 2557, 'when': 2558, 'whenever': 2559, 'whens': 2560, 'where': 2561, 'wheres': 2562, 'whether': 2563, 'which': 2564, 'while': 2565, 'while.': 2566, 'whistling': 2567, 'white': 2568, 'who': 2569, 'who?': 2570, 'whoever': 2571, 'whole': 2572, 'whos': 2573, 'whose': 2574, 'why': 2575, 'wide': 2576, 'wife,': 2577, 'wife.': 2578, 'wifes': 2579, 'will': 2580, 'willing': 2581, 'win.': 2582, 'windmills,': 2583, 'window.': 2584, 'windows.': 2585, 'windsurfing.': 2586, 'wine': 2587, 'wine.': 2588, 'winning': 2589, 'wins.': 2590, 'winter.': 2591, 'winter?': 2592, 'wish': 2593, 'with': 2594, 'without': 2595, 'wolves': 2596, 'woman': 2597, 'woman,': 2598, 'woman.': 2599, 'woman?': 2600, 'women': 2601, 'women,': 2602, 'women.': 2603, 'won': 2604, 'wonder': 2605, 'wondering': 2606, 'wont': 2607, 'wooden': 2608, 'word': 2609, 'word.': 2610, 'words': 2611, 'words.': 2612, 'wore': 2613, 'work': 2614, 'work.': 2615, 'work?': 2616, 'worked': 2617, 'working': 2618, 'working.': 2619, 'works': 2620, 'world': 2621, 'world.': 2622, 'world?': 2623, 'worlds': 2624, 'worried': 2625, 'worried.': 2626, 'worry': 2627, 'worry.': 2628, 'worth': 2629, 'would': 2630, 'wouldnt': 2631, 'wouldve': 2632, 'wrestling': 2633, 'write': 2634, 'write.': 2635, 'writes': 2636, 'writing': 2637, 'written': 2638, 'wrong': 2639, 'wrong.': 2640, 'wrote': 2641, 'yeah.': 2642, 'year,': 2643, 'year.': 2644, 'year?': 2645, 'years': 2646, 'years.': 2647, 'yell': 2648, 'yelling': 2649, 'yellow.': 2650, 'yen.': 2651, 'yesterday': 2652, 'yesterday.': 2653, 'yesterday?': 2654, 'yet': 2655, 'yet.': 2656, 'yet?': 2657, 'yogurt.': 2658, 'yolks.': 2659, 'you': 2660, 'you,': 2661, 'you.': 2662, 'you?': 2663, 'youd': 2664, 'youll': 2665, 'young': 2666, 'young,': 2667, 'younger': 2668, 'younger,': 2669, 'youngest.': 2670, 'your': 2671, 'youre': 2672, 'yours.': 2673, 'yours?': 2674, 'yourself': 2675, 'yourself.': 2676, 'youve': 2677, 'zealand.': 2678, 'zero': 2679, 'zoologist.': 2680, 'zoology': 2681, '€': 2682}\n",
            "3271\n",
            "{0: '<PAD>', 1: '<UNK>', 2: ',', 3: '.', 4: ':', 5: ':.', 6: '?', 7: 'a', 8: 'able', 9: 'aboard.', 10: 'about', 11: 'about?', 12: 'above', 13: 'abroad', 14: 'abroad.', 15: 'abused', 16: 'accept', 17: 'accept.', 18: 'accepted', 19: 'accident.', 20: 'accountant?', 21: 'accurate', 22: 'accurately', 23: 'across', 24: 'act', 25: 'active.', 26: 'actor?', 27: 'actually', 28: 'actually,', 29: 'add?', 30: 'addict.', 31: 'address.', 32: 'admit', 33: 'admitted', 34: 'advance', 35: 'advance.', 36: 'adventures', 37: 'advice,', 38: 'advice.', 39: 'afraid', 40: 'after', 41: 'afternoon.', 42: 'afternoon?', 43: 'again', 44: 'again,', 45: 'again.', 46: 'again?', 47: 'age.', 48: 'aggressive.', 49: 'ago.', 50: 'agree', 51: 'agreed', 52: 'ahead', 53: 'airport', 54: 'alcohol?', 55: 'alice', 56: 'all', 57: 'all.', 58: 'all?', 59: 'allergic', 60: 'allergies.', 61: 'allowed', 62: 'almost', 63: 'alone', 64: 'alone!', 65: 'alone.', 66: 'alone?', 67: 'along', 68: 'already', 69: 'also', 70: 'alternative.', 71: 'always', 72: 'am', 73: 'am.', 74: 'amazing', 75: 'amazing.', 76: 'ambitious.', 77: 'amusing', 78: 'an', 79: 'ancient', 80: 'and', 81: 'angry.', 82: 'animal.', 83: 'animals', 84: 'animals.', 85: 'announced', 86: 'annoying', 87: 'another', 88: 'answer', 89: 'answer.', 90: 'answer?', 91: 'answered.', 92: 'any', 93: 'anybody', 94: 'anybody.', 95: 'anymore.', 96: 'anyone', 97: 'anyone.', 98: 'anything', 99: 'anything.', 100: 'anything?', 101: 'anyway.', 102: 'anyway?', 103: 'anywhere', 104: 'anywhere.', 105: 'apart.', 106: 'apartment', 107: 'apartment.', 108: 'apologize', 109: 'apologize.', 110: 'apologized.', 111: 'appearance.', 112: 'apple', 113: 'apple.', 114: 'apples,', 115: 'apples.', 116: 'application', 117: 'appreciate', 118: 'approved.', 119: 'are', 120: 'are!', 121: 'are.', 122: 'arent', 123: 'argued', 124: 'arizona.', 125: 'arm.', 126: 'army', 127: 'around', 128: 'arrive', 129: 'artist.', 130: 'as', 131: 'ask', 132: 'ask.', 133: 'ask?', 134: 'asked', 135: 'asleep.', 136: 'asleep?', 137: 'assume', 138: 'assure', 139: 'at', 140: 'ate', 141: 'attack!', 142: 'attackers', 143: 'attend', 144: 'attention.', 145: 'attitude.', 146: 'attractive.', 147: 'aunts', 148: 'australia', 149: 'australia,', 150: 'australia.', 151: 'australia?', 152: 'awards.', 153: 'aware', 154: 'away', 155: 'away.', 156: 'awful', 157: 'awkward.', 158: 'b.', 159: 'baby.', 160: 'back', 161: 'back.', 162: 'back?', 163: 'backpack.', 164: 'bad', 165: 'bad?', 166: 'badly.', 167: 'bag', 168: 'bags', 169: 'baking', 170: 'ball.', 171: 'bananas', 172: 'bananas.', 173: 'bangkok', 174: 'bank', 175: 'banker?', 176: 'bankrupt.', 177: 'barking.', 178: 'barks', 179: 'basement.', 180: 'bath', 181: 'bath.', 182: 'batteries.', 183: 'bazaar.', 184: 'be', 185: 'be.', 186: 'be?', 187: 'beans.', 188: 'bear', 189: 'beat', 190: 'beautiful', 191: 'beautiful.', 192: 'beauty', 193: 'became', 194: 'because', 195: 'become', 196: 'bed', 197: 'bed.', 198: 'bee.', 199: 'beef.', 200: 'beef?', 201: 'been', 202: 'beer,', 203: 'beer.', 204: 'beers', 205: 'beers,', 206: 'before', 207: 'before.', 208: 'began', 209: 'begin?', 210: 'beginning', 211: 'begins', 212: 'behave', 213: 'behind', 214: 'beholder.', 215: 'beijing', 216: 'being', 217: 'belgium.', 218: 'believe', 219: 'believed', 220: 'believing', 221: 'bell?', 222: 'below', 223: 'benefits', 224: 'berlin', 225: 'bern', 226: 'best', 227: 'best-selling', 228: 'best.', 229: 'best?', 230: 'bet', 231: 'better', 232: 'better.', 233: 'between', 234: 'bicycle', 235: 'bicycle.', 236: 'bicycle?', 237: 'big', 238: 'bigger.', 239: 'biggest', 240: 'billionaire,', 241: 'billionaires', 242: 'biologist.', 243: 'bird', 244: 'birthday', 245: 'birthday!', 246: 'birthday,', 247: 'birthday.', 248: 'bit', 249: 'bit.', 250: 'bitten', 251: 'bitter.', 252: 'black', 253: 'black.', 254: 'blackboard.', 255: 'blame', 256: 'blames', 257: 'blank.', 258: 'blew', 259: 'blocked', 260: 'blossoms?', 261: 'blue', 262: 'blue.', 263: 'boat.', 264: 'body.', 265: 'book', 266: 'book.', 267: 'booked.', 268: 'booklet', 269: 'books', 270: 'books.', 271: 'bored', 272: 'bored.', 273: 'born', 274: 'born.', 275: 'born?', 276: 'borrow', 277: 'borrowed', 278: 'boston', 279: 'boston.', 280: 'boston?', 281: 'both', 282: 'bother.', 283: 'bottle', 284: 'bottles', 285: 'bought', 286: 'bowline.', 287: 'box.', 288: 'boxes', 289: 'boxing', 290: 'boy.', 291: 'boyfriend', 292: 'boyfriend.', 293: 'boyfriends', 294: 'boys', 295: 'boys.', 296: 'brasilia.', 297: 'brave.', 298: 'braver', 299: 'brazil', 300: 'brazil.', 301: 'bread.', 302: 'bread?', 303: 'break', 304: 'breakfast', 305: 'breakfast.', 306: 'breathe', 307: 'bridge', 308: 'bridge.', 309: 'briefcase.', 310: 'bright', 311: 'bring', 312: 'british', 313: 'broke', 314: 'broke.', 315: 'broken.', 316: 'brother', 317: 'brother.', 318: 'brother?', 319: 'brothers.', 320: 'brought', 321: 'brown', 322: 'brussels', 323: 'bug.', 324: 'bugs.', 325: 'build', 326: 'build.', 327: 'building?', 328: 'buildings.', 329: 'bull.', 330: 'bullfighting?', 331: 'burned', 332: 'bus', 333: 'bus.', 334: 'business', 335: 'busy', 336: 'busy.', 337: 'but', 338: 'buy', 339: 'buy?', 340: 'buying', 341: 'buys', 342: 'by', 343: 'by.', 344: 'c.', 345: 'cab.', 346: 'cake', 347: 'cake,', 348: 'cake.', 349: 'cake?', 350: 'calculator.', 351: 'call', 352: 'call.', 353: 'called', 354: 'calling', 355: 'calm.', 356: 'came', 357: 'came.', 358: 'camera.', 359: 'cameras.', 360: 'can', 361: 'can.', 362: 'canada.', 363: 'canadian', 364: 'canadian.', 365: 'canals.', 366: 'canaries.', 367: 'candle.', 368: 'candy.', 369: 'cannot', 370: 'cant', 371: 'cap.', 372: 'capital', 373: 'capital.', 374: 'captain.', 375: 'car', 376: 'car.', 377: 'car?', 378: 'card.', 379: 'cards.', 380: 'care', 381: 'care,', 382: 'careful.', 383: 'carefully.', 384: 'careless?', 385: 'cares', 386: 'cars', 387: 'cars.', 388: 'cash.', 389: 'cat', 390: 'cat.', 391: 'cats', 392: 'causing', 393: 'cave.', 394: 'celebrate.', 395: 'cell', 396: 'center', 397: 'certain', 398: 'certain.', 399: 'chair', 400: 'chair.', 401: 'challenge', 402: 'chance', 403: 'chance.', 404: 'change', 405: 'change.', 406: 'changed', 407: 'changes.', 408: 'character', 409: 'charismatic', 410: 'charity.', 411: 'chased', 412: 'cheap', 413: 'cheap,', 414: 'cheap.', 415: 'cheaper.', 416: 'checked.', 417: 'cheerful', 418: 'cheese.', 419: 'chemistry', 420: 'cherry', 421: 'chess', 422: 'chess?', 423: 'chicago.', 424: 'chicago?', 425: 'chicken.', 426: 'chicken?', 427: 'children', 428: 'china.', 429: 'chinese.', 430: 'chocolate.', 431: 'choice.', 432: 'choose', 433: 'chopsticks.', 434: 'chose', 435: 'christmas', 436: 'christmas!', 437: 'christmas.', 438: 'chubby', 439: 'chubby.', 440: 'citizen,', 441: 'citizens.', 442: 'city.', 443: 'claims', 444: 'clap?', 445: 'class.', 446: 'classes,', 447: 'classmates.', 448: 'classroom', 449: 'cleaning', 450: 'clear', 451: 'clear.', 452: 'clearly.', 453: 'climate', 454: 'climb', 455: 'climbed', 456: 'clogs', 457: 'close', 458: 'close.', 459: 'close?', 460: 'closed', 461: 'closed.', 462: 'clothes', 463: 'clothes.', 464: 'clothing', 465: 'clouds.', 466: 'club.', 467: 'coach.', 468: 'coat.', 469: 'coffee', 470: 'coffee.', 471: 'coin', 472: 'coins.', 473: 'coke,', 474: 'cold', 475: 'cold.', 476: 'colder', 477: 'colleagues?', 478: 'collect', 479: 'college?', 480: 'color', 481: 'colors.', 482: 'come', 483: 'come?', 484: 'comes', 485: 'comfort.', 486: 'comfortable.', 487: 'coming', 488: 'coming.', 489: 'communication.', 490: 'companies', 491: 'company.', 492: 'compare', 493: 'competent.', 494: 'competitive.', 495: 'complain.', 496: 'complained', 497: 'complaints.', 498: 'complex.', 499: 'complicated.', 500: 'computer', 501: 'conceive', 502: 'concentrate', 503: 'concerned', 504: 'concert', 505: 'conference', 506: 'confused.', 507: 'confusing', 508: 'congratulations!', 509: 'cons.', 510: 'conservative.', 511: 'constant', 512: 'contact', 513: 'continue', 514: 'continue.', 515: 'contribute', 516: 'control.', 517: 'convalescing', 518: 'convince', 519: 'convinced', 520: 'convincing.', 521: 'cook', 522: 'cook.', 523: 'cookbooks.', 524: 'cooked?', 525: 'cookie.', 526: 'cookies.', 527: 'cooking.', 528: 'cooking?', 529: 'cool', 530: 'cooler', 531: 'cooperative.', 532: 'correct', 533: 'correct?', 534: 'cost', 535: 'cost?', 536: 'cough', 537: 'could', 538: 'couldnt', 539: 'countries', 540: 'countries.', 541: 'couple', 542: 'couple?', 543: 'courageous.', 544: 'cousins.', 545: 'covered', 546: 'cows', 547: 'crackers.', 548: 'cranky.', 549: 'crazy.', 550: 'cream.', 551: 'creative.', 552: 'cried?', 553: 'criminal.', 554: 'crowded', 555: 'cry', 556: 'cry.', 557: 'crying.', 558: 'cultures.', 559: 'cup', 560: 'cure', 561: 'curious.', 562: 'curry', 563: 'cut', 564: 'cycling.', 565: 'dad', 566: 'dad.', 567: 'daiquiri.', 568: 'damaged', 569: 'dance', 570: 'dance,', 571: 'danced', 572: 'danced.', 573: 'dangerous', 574: 'dark', 575: 'dark.', 576: 'darker', 577: 'date', 578: 'dating', 579: 'daughter.', 580: 'daughters', 581: 'day', 582: 'day!', 583: 'day.', 584: 'day?', 585: 'days', 586: 'days.', 587: 'days?', 588: 'dazed', 589: 'dead', 590: 'dead.', 591: 'deal.', 592: 'debt.', 593: 'debt?', 594: 'decide', 595: 'decide.', 596: 'decided', 597: 'decided.', 598: 'deeply.', 599: 'defeat', 600: 'defeated.', 601: 'definitely', 602: 'degree.', 603: 'delete', 604: 'delicious.', 605: 'delightful.', 606: 'depend', 607: 'derailed', 608: 'deserve', 609: 'desk.', 610: 'dessert.', 611: 'dessert?', 612: 'destroyed', 613: 'details.', 614: 'detained', 615: 'detergent.', 616: 'developed', 617: 'did', 618: 'did.', 619: 'didnt', 620: 'die', 621: 'died', 622: 'died.', 623: 'dieting.', 624: 'difference', 625: 'difficult', 626: 'difficult.', 627: 'dinner.', 628: 'director.', 629: 'disappeared', 630: 'disappointed?', 631: 'disco', 632: 'discussed', 633: 'dishes.', 634: 'disliked', 635: 'distance', 636: 'divorced.', 637: 'do', 638: 'do,', 639: 'do.', 640: 'do?', 641: 'doctor', 642: 'doctor,', 643: 'doctor.', 644: 'doctors.', 645: 'does', 646: 'does.', 647: 'doesnt', 648: 'doesnt.', 649: 'dog', 650: 'dog,', 651: 'dog.', 652: 'dogs.', 653: 'doing', 654: 'doing.', 655: 'dollars', 656: 'dollars!', 657: 'dollars.', 658: 'done', 659: 'done.', 660: 'dont', 661: 'door', 662: 'door.', 663: 'door?', 664: 'doors.', 665: 'doubt', 666: 'down', 667: 'down.', 668: 'downloaded', 669: 'downstairs', 670: 'downstairs.', 671: 'drank', 672: 'drank,', 673: 'draw', 674: 'drawer', 675: 'dream', 676: 'dream.', 677: 'dress', 678: 'dress.', 679: 'drink', 680: 'drink.', 681: 'drink?', 682: 'drinking', 683: 'drinks', 684: 'drive', 685: 'drive.', 686: 'driver', 687: 'driver.', 688: 'driving', 689: 'driving?', 690: 'drops?', 691: 'drugs.', 692: 'drummer', 693: 'drunk', 694: 'drunk.', 695: 'ducks', 696: 'dumb.', 697: 'durian.', 698: 'duties?', 699: 'dying', 700: 'dying.', 701: 'each', 702: 'earlier', 703: 'earned', 704: 'easier.', 705: 'easily.', 706: 'easy', 707: 'easy.', 708: 'eat', 709: 'eat.', 710: 'eat?', 711: 'eaten', 712: 'eating', 713: 'eating.', 714: 'eats', 715: 'egg', 716: 'eggs', 717: 'egypt.', 718: 'eight.', 719: 'eighteen', 720: 'eighteen.', 721: 'either', 722: 'elderly.', 723: 'else', 724: 'else.', 725: 'emphasize', 726: 'empty', 727: 'empty.', 728: 'encourage', 729: 'enemy', 730: 'engine.', 731: 'england.', 732: 'english', 733: 'english.', 734: 'english?', 735: 'enjoy', 736: 'enjoyed', 737: 'enjoying', 738: 'enough', 739: 'enough.', 740: 'enter', 741: 'error.', 742: 'escaped', 743: 'especially', 744: 'essay.', 745: 'even', 746: 'evening.', 747: 'evening?', 748: 'ever', 749: 'ever.', 750: 'everest', 751: 'everest.', 752: 'every', 753: 'everybody', 754: 'everybody.', 755: 'everyone', 756: 'everyone!', 757: 'everyone.', 758: 'everything', 759: 'everything.', 760: 'everything?', 761: 'everythings', 762: 'everywhere.', 763: 'exactly', 764: 'exam', 765: 'example?', 766: 'exams', 767: 'excellent.', 768: 'except', 769: 'exceptions.', 770: 'exchange', 771: 'exchanging', 772: 'excited.', 773: 'exciting.', 774: 'excuse', 775: 'exercise.', 776: 'exist.', 777: 'expect', 778: 'expect.', 779: 'expected', 780: 'expecting', 781: 'expensive', 782: 'explain', 783: 'explosion', 784: 'express', 785: 'extra', 786: 'extremely', 787: 'eye', 788: 'eyes', 789: 'eyes.', 790: 'eyesight.', 791: 'face', 792: 'face.', 793: 'facing', 794: 'fact', 795: 'facts!', 796: 'fake.', 797: 'fall.', 798: 'fall?', 799: 'fallen', 800: 'falling.', 801: 'family', 802: 'family.', 803: 'famous', 804: 'famous.', 805: 'famous?', 806: 'far', 807: 'far.', 808: 'far?', 809: 'fast', 810: 'fast.', 811: 'faster', 812: 'faster.', 813: 'father', 814: 'father.', 815: 'fathers', 816: 'fault?', 817: 'favor.', 818: 'favorite', 819: 'february', 820: 'feel', 821: 'feeling', 822: 'feeling.', 823: 'feels', 824: 'fell', 825: 'felt', 826: 'few', 827: 'fight', 828: 'fight.', 829: 'figure', 830: 'files.', 831: 'filled', 832: 'final', 833: 'finally', 834: 'financial', 835: 'find', 836: 'fine,', 837: 'finish', 838: 'finish.', 839: 'finished', 840: 'finished.', 841: 'finland.', 842: 'fire', 843: 'fire.', 844: 'first', 845: 'first.', 846: 'fish', 847: 'fish.', 848: 'fish?', 849: 'fishing.', 850: 'five', 851: 'five.', 852: 'fix', 853: 'fixed', 854: 'flashlight', 855: 'flew', 856: 'flowers', 857: 'flowers.', 858: 'flown?', 859: 'fluently?', 860: 'fly', 861: 'fog.', 862: 'folk', 863: 'follow', 864: 'food', 865: 'food.', 866: 'food?', 867: 'foolish.', 868: 'foot.', 869: 'for', 870: 'for,', 871: 'for.', 872: 'for?', 873: 'foreign', 874: 'foreigner.', 875: 'foreigners?', 876: 'forever', 877: 'forget', 878: 'forget.', 879: 'forgive', 880: 'forgot', 881: 'forgot.', 882: 'forgotten', 883: 'former', 884: 'fortune', 885: 'forty.', 886: 'forward', 887: 'found', 888: 'fountain', 889: 'four', 890: 'france.', 891: 'free', 892: 'french', 893: 'french,', 894: 'french.', 895: 'french?', 896: 'fresh', 897: 'freshman.', 898: 'fried', 899: 'friend', 900: 'friend.', 901: 'friendly', 902: 'friendly.', 903: 'friends', 904: 'friends.', 905: 'fries.', 906: 'from', 907: 'from?', 908: 'front', 909: 'fruit.', 910: 'fuji', 911: 'fuji.', 912: 'full', 913: 'full.', 914: 'fully', 915: 'fun', 916: 'fun.', 917: 'funny', 918: 'funny.', 919: 'further.', 920: 'fuse?', 921: 'future.', 922: 'future?', 923: 'game', 924: 'game?', 925: 'games.', 926: 'gas.', 927: 'gate', 928: 'gate.', 929: 'gave', 930: 'generally', 931: 'gentle', 932: 'gentleman.', 933: 'germany', 934: 'germany.', 935: 'get', 936: 'get,', 937: 'gets', 938: 'getting', 939: 'ghosts', 940: 'gift.', 941: 'gingerbread.', 942: 'girl', 943: 'girl.', 944: 'girlfriend', 945: 'girlfriend.', 946: 'girls', 947: 'give', 948: 'glad', 949: 'glasses.', 950: 'go', 951: 'go,', 952: 'go.', 953: 'go?', 954: 'goal.', 955: 'godmother.', 956: 'goes', 957: 'going', 958: 'going?', 959: 'gold', 960: 'golf?', 961: 'gone', 962: 'gone.', 963: 'gone?', 964: 'good', 965: 'good-looking.', 966: 'good.', 967: 'goodbye.', 968: 'got', 969: 'gotten', 970: 'grabbed', 971: 'grade.', 972: 'gradually,', 973: 'graduated', 974: 'grammar', 975: 'grandfather', 976: 'grandmother', 977: 'grass.', 978: 'grateful', 979: 'great', 980: 'great-grandmother.', 981: 'greatest', 982: 'grew', 983: 'group', 984: 'grow', 985: 'grows', 986: 'guess', 987: 'guess.', 988: 'guide.', 989: 'guilty', 990: 'guitar.', 991: 'gun.', 992: 'guy,', 993: 'guy.', 994: 'guys', 995: 'habits?', 996: 'had', 997: 'hadnt', 998: 'hair.', 999: 'haircut,', 1000: 'half', 1001: 'half-brother.', 1002: 'halloween', 1003: 'hamburgers', 1004: 'hammer?', 1005: 'hand', 1006: 'hand.', 1007: 'handle', 1008: 'hands', 1009: 'hands.', 1010: 'happen', 1011: 'happen,', 1012: 'happen?', 1013: 'happened', 1014: 'happened.', 1015: 'happening', 1016: 'happens.', 1017: 'happy', 1018: 'happy.', 1019: 'hard', 1020: 'hard.', 1021: 'harvard.', 1022: 'has', 1023: 'hasnt', 1024: 'hat', 1025: 'hat,', 1026: 'hat.', 1027: 'hate', 1028: 'hated', 1029: 'hates', 1030: 'haunted.', 1031: 'have', 1032: 'have.', 1033: 'havent', 1034: 'having', 1035: 'hayashi', 1036: 'he', 1037: 'he?', 1038: 'head', 1039: 'headache', 1040: 'headache.', 1041: 'headed', 1042: 'headline', 1043: 'health', 1044: 'healthy', 1045: 'hear', 1046: 'heard', 1047: 'heart.', 1048: 'heat.', 1049: 'heaven.', 1050: 'heavier,', 1051: 'heavy.', 1052: 'hed', 1053: 'heights.', 1054: 'hello!', 1055: 'help', 1056: 'help.', 1057: 'help?', 1058: 'helped', 1059: 'helpful.', 1060: 'helping', 1061: 'helsinki', 1062: 'her', 1063: 'her.', 1064: 'here', 1065: 'here,', 1066: 'here.', 1067: 'here?', 1068: 'herself?', 1069: 'hes', 1070: 'hi,', 1071: 'hiccups.', 1072: 'hidden', 1073: 'hide.', 1074: 'hiding', 1075: 'high', 1076: 'highest', 1077: 'highly', 1078: 'hiking.', 1079: 'him', 1080: 'him.', 1081: 'himself', 1082: 'himself.', 1083: 'hippopotamus', 1084: 'hire', 1085: 'his', 1086: 'history', 1087: 'history!', 1088: 'hit', 1089: 'hobby.', 1090: 'hold', 1091: 'holding', 1092: 'holiday', 1093: 'home', 1094: 'home.', 1095: 'homework', 1096: 'homework.', 1097: 'honestly.', 1098: 'hope', 1099: 'hoping', 1100: 'horizon.', 1101: 'hospital', 1102: 'hospital,', 1103: 'hospital.', 1104: 'hot', 1105: 'hotel.', 1106: 'hour.', 1107: 'hours', 1108: 'hours.', 1109: 'house', 1110: 'house.', 1111: 'house?', 1112: 'houses', 1113: 'how', 1114: 'hows', 1115: 'human.', 1116: 'humiliating.', 1117: 'hunger.', 1118: 'hungry,', 1119: 'hungry.', 1120: 'hunting.', 1121: 'hurry', 1122: 'hurry.', 1123: 'hurt', 1124: 'hurt.', 1125: 'hurts.', 1126: 'husband.', 1127: 'husbands', 1128: 'i', 1129: 'i?', 1130: 'ice', 1131: 'id', 1132: 'idea', 1133: 'idea.', 1134: 'ideas', 1135: 'ideas.', 1136: 'identify', 1137: 'if', 1138: 'ignore', 1139: 'ill', 1140: 'ill.', 1141: 'im', 1142: 'im,', 1143: 'imagining', 1144: 'immediately.', 1145: 'import', 1146: 'important', 1147: 'important.', 1148: 'impossible.', 1149: 'impress', 1150: 'impressed.', 1151: 'impressive.', 1152: 'improve', 1153: 'in', 1154: 'in.', 1155: 'in?', 1156: 'incentive.', 1157: 'including', 1158: 'india.', 1159: 'inevitable.', 1160: 'influential.', 1161: 'injured.', 1162: 'insects.', 1163: 'insecure', 1164: 'inside.', 1165: 'instead.', 1166: 'intelligent', 1167: 'intends', 1168: 'interested', 1169: 'interested.', 1170: 'interesting', 1171: 'internet.', 1172: 'intersection.', 1173: 'into', 1174: 'involved', 1175: 'involved.', 1176: 'irish', 1177: 'iron', 1178: 'is', 1179: 'is.', 1180: 'is?', 1181: 'island.', 1182: 'isnt', 1183: 'isnt.', 1184: 'issues?', 1185: 'it', 1186: 'it,', 1187: 'it.', 1188: 'it?', 1189: 'italy', 1190: 'itll', 1191: 'its', 1192: 'ive', 1193: 'jackson', 1194: 'jackson.', 1195: 'jackson?', 1196: 'jail.', 1197: 'japan', 1198: 'japan.', 1199: 'japanese', 1200: 'japanese.', 1201: 'japanese?', 1202: 'jazz.', 1203: 'jealous,', 1204: 'jeans.', 1205: 'job', 1206: 'job.', 1207: 'jobs', 1208: 'john.', 1209: 'join', 1210: 'joined', 1211: 'joke', 1212: 'joke,', 1213: 'judged', 1214: 'juice.', 1215: 'july.', 1216: 'jumped', 1217: 'just', 1218: 'keep', 1219: 'kept', 1220: 'ketchup', 1221: 'key.', 1222: 'keys', 1223: 'kid', 1224: 'kid.', 1225: 'kidding!', 1226: 'kids', 1227: 'kids,', 1228: 'killed', 1229: 'killed.', 1230: 'kilometers', 1231: 'kind', 1232: 'kind.', 1233: 'king', 1234: 'kiss', 1235: 'kissed', 1236: 'kite.', 1237: 'kitten.', 1238: 'knew', 1239: 'knife', 1240: 'knocked', 1241: 'know', 1242: 'know,', 1243: 'know.', 1244: 'knowing', 1245: 'known', 1246: 'knows', 1247: 'knows.', 1248: 'korean', 1249: 'kosher', 1250: 'kyiv', 1251: 'kyiv.', 1252: 'lamp', 1253: 'language', 1254: 'language.', 1255: 'languages', 1256: 'languages,', 1257: 'languages.', 1258: 'laptop', 1259: 'laptop,', 1260: 'laptop?', 1261: 'large', 1262: 'large.', 1263: 'larger', 1264: 'larger.', 1265: 'last', 1266: 'late', 1267: 'late.', 1268: 'lately?', 1269: 'later?', 1270: 'latin.', 1271: 'laugh.', 1272: 'laughing.', 1273: 'laughs', 1274: 'laughter.', 1275: 'laundry', 1276: 'law', 1277: 'lazy.', 1278: 'lead', 1279: 'learn', 1280: 'learn.', 1281: 'learned', 1282: 'learning', 1283: 'least', 1284: 'leather.', 1285: 'leave', 1286: 'leave.', 1287: 'leaves', 1288: 'leaving', 1289: 'left', 1290: 'left.', 1291: 'legs.', 1292: 'lend', 1293: 'lent', 1294: 'let', 1295: 'lets', 1296: 'letter', 1297: 'letter.', 1298: 'letter?', 1299: 'letters.', 1300: 'library', 1301: 'lie.', 1302: 'lied', 1303: 'life', 1304: 'lifetime.', 1305: 'light', 1306: 'light.', 1307: 'lightning', 1308: 'like', 1309: 'like.', 1310: 'like?', 1311: 'liked', 1312: 'likely', 1313: 'likes', 1314: 'limping', 1315: 'line', 1316: 'lion.', 1317: 'lips.', 1318: 'liquid', 1319: 'list', 1320: 'list.', 1321: 'listed.', 1322: 'listen.', 1323: 'listened', 1324: 'listening', 1325: 'listening.', 1326: 'lit', 1327: 'liter', 1328: 'literally', 1329: 'little', 1330: 'little.', 1331: 'little?', 1332: 'live', 1333: 'live.', 1334: 'lived', 1335: 'lived.', 1336: 'lives', 1337: 'lives.', 1338: 'living', 1339: 'loan.', 1340: 'local?', 1341: 'locked', 1342: 'log', 1343: 'london', 1344: 'lonely.', 1345: 'long', 1346: 'long.', 1347: 'long?', 1348: 'longer', 1349: 'longer.', 1350: 'longer?', 1351: 'look', 1352: 'look.', 1353: 'looked', 1354: 'looking', 1355: 'looks', 1356: 'lose', 1357: 'lose.', 1358: 'loss', 1359: 'lost', 1360: 'lost.', 1361: 'lot', 1362: 'lot.', 1363: 'louder.', 1364: 'love', 1365: 'love.', 1366: 'loved', 1367: 'lovely!', 1368: 'lover.', 1369: 'loves', 1370: 'low.', 1371: 'lower', 1372: 'lucky', 1373: 'lucky.', 1374: 'lunch', 1375: 'lunch.', 1376: 'lunch?', 1377: 'lying', 1378: 'lying.', 1379: 'machine.', 1380: 'mad.', 1381: 'made', 1382: 'made.', 1383: 'made?', 1384: 'madrid', 1385: 'mail', 1386: 'mailbox', 1387: 'make', 1388: 'makes', 1389: 'making', 1390: 'man', 1391: 'man.', 1392: 'man?', 1393: 'many', 1394: 'map', 1395: 'map.', 1396: 'marine', 1397: 'mark', 1398: 'married', 1399: 'married.', 1400: 'marry', 1401: 'marshmallows?', 1402: 'mary', 1403: 'mary.', 1404: 'mary?', 1405: 'marys', 1406: 'marys.', 1407: 'mask.', 1408: 'material.', 1409: 'math', 1410: 'mathematics', 1411: 'mathematics.', 1412: 'matter', 1413: 'matter.', 1414: 'may', 1415: 'maybe', 1416: 'me', 1417: 'me,', 1418: 'me.', 1419: 'me?', 1420: 'mean', 1421: 'meaning', 1422: 'means', 1423: 'meat.', 1424: 'meat?', 1425: 'medicine.', 1426: 'meet', 1427: 'meeting.', 1428: 'meeting?', 1429: 'meetings.', 1430: 'member.', 1431: 'men', 1432: 'men.', 1433: 'merry', 1434: 'message.', 1435: 'met', 1436: 'met.', 1437: 'mexican', 1438: 'microwave', 1439: 'middle', 1440: 'might', 1441: 'mightve', 1442: 'miles', 1443: 'milk', 1444: 'milk.', 1445: 'million', 1446: 'millionaire.', 1447: 'mind', 1448: 'mine.', 1449: 'minute.', 1450: 'minutes', 1451: 'minutes.', 1452: 'misleading.', 1453: 'miss', 1454: 'mistake.', 1455: 'mistaken.', 1456: 'mistakes.', 1457: 'model', 1458: 'model.', 1459: 'modest', 1460: 'moment.', 1461: 'monday', 1462: 'monday.', 1463: 'mondays?', 1464: 'money', 1465: 'money.', 1466: 'money?', 1467: 'mongolia.', 1468: 'month', 1469: 'month.', 1470: 'months', 1471: 'mood.', 1472: 'mood?', 1473: 'moon', 1474: 'more', 1475: 'more.', 1476: 'more?', 1477: 'morning', 1478: 'morning.', 1479: 'mosquitoes', 1480: 'most', 1481: 'mother', 1482: 'mother.', 1483: 'motor', 1484: 'motorcycle?', 1485: 'mount', 1486: 'mountain', 1487: 'mountaintops', 1488: 'mouth', 1489: 'moved', 1490: 'movie.', 1491: 'movie?', 1492: 'movies', 1493: 'movies.', 1494: 'mt.', 1495: 'much', 1496: 'much,', 1497: 'much.', 1498: 'mug', 1499: 'mushrooms', 1500: 'music.', 1501: 'musician.', 1502: 'must', 1503: 'my', 1504: 'myself', 1505: 'myself.', 1506: 'naka-meguro.', 1507: 'name', 1508: 'name.', 1509: 'name?', 1510: 'names', 1511: 'names.', 1512: 'names?', 1513: 'narrow.', 1514: 'naughty', 1515: 'near', 1516: 'nearby.', 1517: 'nearly', 1518: 'necessary.', 1519: 'need', 1520: 'need?', 1521: 'needed', 1522: 'needs', 1523: 'neighbor?', 1524: 'neighborhood', 1525: 'neither', 1526: 'nervous', 1527: 'nervous.', 1528: 'netherlands', 1529: 'never', 1530: 'new', 1531: 'new.', 1532: 'next', 1533: 'next.', 1534: 'next?', 1535: 'nice', 1536: 'nice.', 1537: 'nickname.', 1538: 'night', 1539: 'night,', 1540: 'night.', 1541: 'night?', 1542: 'nine.', 1543: 'no', 1544: 'nobel', 1545: 'nobody', 1546: 'noisy', 1547: 'nor', 1548: 'normal', 1549: 'not', 1550: 'not.', 1551: 'notebook?', 1552: 'notes.', 1553: 'nothing', 1554: 'notice', 1555: 'notice.', 1556: 'novels.', 1557: 'now', 1558: 'now,', 1559: 'now.', 1560: 'now?', 1561: 'nowadays', 1562: 'nowadays.', 1563: 'number', 1564: 'number.', 1565: 'numbers', 1566: 'nursing', 1567: 'nutritious.', 1568: 'obnoxious.', 1569: 'obvious.', 1570: 'obviously', 1571: 'obviously,', 1572: 'ocean', 1573: 'oclock.', 1574: 'october', 1575: 'october.', 1576: 'of', 1577: 'off', 1578: 'off.', 1579: 'off?', 1580: 'offended.', 1581: 'offered', 1582: 'office', 1583: 'office.', 1584: 'office?', 1585: 'often', 1586: 'often?', 1587: 'oh,', 1588: 'ok', 1589: 'ok,', 1590: 'ok.', 1591: 'ok?', 1592: 'okay.', 1593: 'old', 1594: 'old.', 1595: 'older', 1596: 'on', 1597: 'on.', 1598: 'on?', 1599: 'once', 1600: 'once.', 1601: 'one', 1602: 'one.', 1603: 'one?', 1604: 'ones.', 1605: 'only', 1606: 'open', 1607: 'open.', 1608: 'opened', 1609: 'opens', 1610: 'opinion', 1611: 'opinion.', 1612: 'or', 1613: 'orange.', 1614: 'oranges', 1615: 'ostriches', 1616: 'other', 1617: 'other.', 1618: 'otherwise.', 1619: 'ottawa', 1620: 'ought', 1621: 'our', 1622: 'out', 1623: 'out.', 1624: 'outside', 1625: 'outside.', 1626: 'oven.', 1627: 'over', 1628: 'over.', 1629: 'overreact.', 1630: 'overseas', 1631: 'overseas.', 1632: 'overweight.', 1633: 'own', 1634: 'pack', 1635: 'page', 1636: 'paid', 1637: 'paid.', 1638: 'pain.', 1639: 'pain?', 1640: 'painful', 1641: 'paint?', 1642: 'painted', 1643: 'palace.', 1644: 'pancakes.', 1645: 'papers', 1646: 'parents', 1647: 'parents.', 1648: 'paris', 1649: 'park', 1650: 'park.', 1651: 'parked', 1652: 'parking', 1653: 'part', 1654: 'particularly', 1655: 'party.', 1656: 'passed', 1657: 'passengers', 1658: 'passport,', 1659: 'pattern.', 1660: 'pay', 1661: 'paying', 1662: 'payment', 1663: 'peach', 1664: 'peak', 1665: 'peas.', 1666: 'pen', 1667: 'pencil.', 1668: 'pens.', 1669: 'people', 1670: 'people,', 1671: 'people.', 1672: 'per', 1673: 'perfect', 1674: 'person', 1675: 'person.', 1676: 'pet.', 1677: 'phoenix', 1678: 'phone', 1679: 'phone.', 1680: 'phone?', 1681: 'photographs', 1682: 'pick', 1683: 'picture', 1684: 'picture.', 1685: 'picture?', 1686: 'piece', 1687: 'pig', 1688: 'pig.', 1689: 'pizza', 1690: 'pizza?', 1691: 'place', 1692: 'place,', 1693: 'place.', 1694: 'plan', 1695: 'plan.', 1696: 'plane', 1697: 'planes', 1698: 'planes.', 1699: 'planning', 1700: 'plans', 1701: 'plans.', 1702: 'plans?', 1703: 'planted', 1704: 'plants', 1705: 'plants?', 1706: 'play', 1707: 'played', 1708: 'player', 1709: 'player.', 1710: 'playing', 1711: 'plays', 1712: 'please', 1713: 'please.', 1714: 'please?', 1715: 'pleased', 1716: 'pleased.', 1717: 'plumber.', 1718: 'pocket.', 1719: 'pocketknife.', 1720: 'poem', 1721: 'poems.', 1722: 'point', 1723: 'point.', 1724: 'pointing', 1725: 'poland', 1726: 'police', 1727: 'police.', 1728: 'policeman', 1729: 'polite.', 1730: 'pond.', 1731: 'poor', 1732: 'poor.', 1733: 'pop', 1734: 'popular', 1735: 'pork,', 1736: 'pork.', 1737: 'positive', 1738: 'possible,', 1739: 'potential.', 1740: 'power.', 1741: 'powerful.', 1742: 'practical.', 1743: 'pray', 1744: 'predict', 1745: 'predictable.', 1746: 'prefer', 1747: 'preparations', 1748: 'pressure', 1749: 'pretended', 1750: 'pretty', 1751: 'pretty.', 1752: 'price', 1753: 'price?', 1754: 'prices?', 1755: 'prime', 1756: 'prison', 1757: 'prize,', 1758: 'proactive.', 1759: 'probably', 1760: 'problem', 1761: 'problem.', 1762: 'problem?', 1763: 'problems.', 1764: 'process', 1765: 'produces', 1766: 'products', 1767: 'professor.', 1768: 'promised', 1769: 'pronouncing', 1770: 'proof', 1771: 'proof.', 1772: 'pros', 1773: 'protect', 1774: 'proved', 1775: 'public', 1776: 'public.', 1777: 'pulled', 1778: 'punch', 1779: 'puppy', 1780: 'purpose.', 1781: 'put', 1782: 'question', 1783: 'question.', 1784: 'question?', 1785: 'questions', 1786: 'questions.', 1787: 'quicker', 1788: 'quickly', 1789: 'quickly.', 1790: 'quiet', 1791: 'quiet.', 1792: 'quietly.', 1793: 'quit', 1794: 'quit.', 1795: 'quit?', 1796: 'quite', 1797: 'radio', 1798: 'radio.', 1799: 'radio?', 1800: 'rain', 1801: 'rain.', 1802: 'rained', 1803: 'raining', 1804: 'raining.', 1805: 'raise', 1806: 'ramen.', 1807: 'ran', 1808: 'rang', 1809: 'rare.', 1810: 'rate?', 1811: 'rather', 1812: 'reach', 1813: 'read', 1814: 'read.', 1815: 'read?', 1816: 'reading', 1817: 'reading.', 1818: 'ready', 1819: 'ready.', 1820: 'real', 1821: 'realize', 1822: 'realized', 1823: 'really', 1824: 'really?', 1825: 'received', 1826: 'recent', 1827: 'red', 1828: 'red.', 1829: 'refrigerator.', 1830: 'refused', 1831: 'regrets.', 1832: 'relationship', 1833: 'relatives.', 1834: 'relax.', 1835: 'reluctant', 1836: 'remained', 1837: 'remember', 1838: 'remembers', 1839: 'remind', 1840: 'rent', 1841: 'rent.', 1842: 'repair', 1843: 'repeat', 1844: 'repeated', 1845: 'rephrase', 1846: 'replaceable.', 1847: 'reply.', 1848: 'report', 1849: 'report.', 1850: 'report?', 1851: 'reprimanded', 1852: 'requested,', 1853: 'required', 1854: 'respect', 1855: 'respect.', 1856: 'response', 1857: 'rest', 1858: 'restaurant', 1859: 'restaurant.', 1860: 'restroom?', 1861: 'results', 1862: 'return', 1863: 'rewarded.', 1864: 'rice', 1865: 'rice,', 1866: 'rice.', 1867: 'rich', 1868: 'rich.', 1869: 'ride', 1870: 'riding.', 1871: 'right', 1872: 'right.', 1873: 'right?', 1874: 'ring', 1875: 'rings', 1876: 'risk.', 1877: 'risky', 1878: 'risky.', 1879: 'rival.', 1880: 'river.', 1881: 'robbed', 1882: 'rock', 1883: 'rolled', 1884: 'rome.', 1885: 'roof', 1886: 'room', 1887: 'room.', 1888: 'roommate.', 1889: 'rooms', 1890: 'rosemary.', 1891: 'roses', 1892: 'roses.', 1893: 'round', 1894: 'rude', 1895: 'rude.', 1896: 'rule,', 1897: 'rule.', 1898: 'rumored', 1899: 'run', 1900: 'run.', 1901: 'run?', 1902: 'runner.', 1903: 'running.', 1904: 'sacred', 1905: 'sad.', 1906: 'safe', 1907: 'safe.', 1908: 'said', 1909: 'said.', 1910: 'sale', 1911: 'same', 1912: 'same.', 1913: 'sandwiches', 1914: 'sandwiches.', 1915: 'sang', 1916: 'santas', 1917: 'sashimi.', 1918: 'sat', 1919: 'save', 1920: 'saw', 1921: 'say', 1922: 'say.', 1923: 'say?', 1924: 'saying.', 1925: 'says', 1926: 'says.', 1927: 'scare', 1928: 'scared', 1929: 'scary.', 1930: 'school', 1931: 'school,', 1932: 'school.', 1933: 'school?', 1934: 'schoolbooks.', 1935: 'scored', 1936: 'scripts.', 1937: 'sea', 1938: 'searched', 1939: 'searching', 1940: 'seat', 1941: 'secret.', 1942: 'secret?', 1943: 'see', 1944: 'seem', 1945: 'seemed', 1946: 'seems', 1947: 'seen', 1948: 'seen.', 1949: 'send', 1950: 'senior', 1951: 'seniority.', 1952: 'sense.', 1953: 'sent', 1954: 'sentence?', 1955: 'serious', 1956: 'serious.', 1957: 'seriously', 1958: 'served', 1959: 'set', 1960: 'setting', 1961: 'seven', 1962: 'several', 1963: 'sewing', 1964: 'shake', 1965: 'shall', 1966: 'shallow,', 1967: 'shape', 1968: 'shape.', 1969: 'share.', 1970: 'sharkskin', 1971: 'sharp.', 1972: 'she', 1973: 'she?', 1974: 'sheep', 1975: 'shibuya', 1976: 'shirt.', 1977: 'shirts', 1978: 'shirtsleeves', 1979: 'shirtsleeves.', 1980: 'shock.', 1981: 'shocked.', 1982: 'shoe', 1983: 'shoes.', 1984: 'shoot', 1985: 'shop', 1986: 'shop.', 1987: 'shopping.', 1988: 'short', 1989: 'short.', 1990: 'shortest', 1991: 'shot.', 1992: 'should', 1993: 'shouldnt', 1994: 'shouldve', 1995: 'shouldve.', 1996: 'show', 1997: 'showed', 1998: 'showing', 1999: 'sick', 2000: 'sick.', 2001: 'side', 2002: 'side.', 2003: 'sightseeing.', 2004: 'sign', 2005: 'silver', 2006: 'simple', 2007: 'simple.', 2008: 'since', 2009: 'sing?', 2010: 'singer', 2011: 'singer?', 2012: 'singing?', 2013: 'single', 2014: 'sir?', 2015: 'sister', 2016: 'sister?', 2017: 'sisters', 2018: 'sisters?', 2019: 'sit', 2020: 'sitting', 2021: 'situation', 2022: 'six-thirty.', 2023: 'sixteen', 2024: 'size', 2025: 'size.', 2026: 'skiing.', 2027: 'skin', 2028: 'skip', 2029: 'skipper.', 2030: 'sleep', 2031: 'sleep.', 2032: 'sleeping', 2033: 'sleeping.', 2034: 'sleeping?', 2035: 'sleepy,', 2036: 'sleepy.', 2037: 'sleepy?', 2038: 'slept', 2039: 'slight', 2040: 'slow.', 2041: 'slowly', 2042: 'slowly.', 2043: 'small', 2044: 'small.', 2045: 'smart', 2046: 'smart.', 2047: 'smartest', 2048: 'smell', 2049: 'smile.', 2050: 'smiling.', 2051: 'smoke.', 2052: 'snake.', 2053: 'snow', 2054: 'snowing', 2055: 'so', 2056: 'so.', 2057: 'soccer', 2058: 'socks.', 2059: 'sold', 2060: 'solve', 2061: 'some', 2062: 'some.', 2063: 'someone', 2064: 'something', 2065: 'something.', 2066: 'sometimes', 2067: 'somewhat', 2068: 'somewhere', 2069: 'somewhere,', 2070: 'son', 2071: 'son-in-law.', 2072: 'son.', 2073: 'song', 2074: 'song,', 2075: 'song.', 2076: 'soon', 2077: 'soon.', 2078: 'sorry', 2079: 'sorry,', 2080: 'sorry.', 2081: 'sound', 2082: 'sour.', 2083: 'source', 2084: 'spain.', 2085: 'speak', 2086: 'speak.', 2087: 'speak?', 2088: 'speaking', 2089: 'speaking.', 2090: 'speaking?', 2091: 'speaks', 2092: 'special', 2093: 'speeches', 2094: 'speed', 2095: 'spell', 2096: 'spend', 2097: 'spends', 2098: 'spent', 2099: 'spider.', 2100: 'spilled', 2101: 'spoken', 2102: 'sports.', 2103: 'spots', 2104: 'sprang', 2105: 'spread', 2106: 'staff', 2107: 'standing', 2108: 'staring', 2109: 'start?', 2110: 'started', 2111: 'started.', 2112: 'starved.', 2113: 'starving.', 2114: 'states.', 2115: 'station', 2116: 'station?', 2117: 'stay', 2118: 'stay.', 2119: 'stay?', 2120: 'stayed', 2121: 'staying', 2122: 'steak.', 2123: 'steal', 2124: 'still', 2125: 'stole', 2126: 'stolen', 2127: 'stop', 2128: 'stop.', 2129: 'stopped', 2130: 'stopping?', 2131: 'store', 2132: 'store.', 2133: 'story', 2134: 'story.', 2135: 'stove.', 2136: 'straight', 2137: 'strange', 2138: 'stranger?', 2139: 'strapless', 2140: 'street', 2141: 'street.', 2142: 'street?', 2143: 'stretch', 2144: 'stretch.', 2145: 'strict,', 2146: 'strong.', 2147: 'strong?', 2148: 'stronger,', 2149: 'student', 2150: 'student.', 2151: 'students', 2152: 'students.', 2153: 'study', 2154: 'studying', 2155: 'studying.', 2156: 'studying?', 2157: 'stuff', 2158: 'stupid.', 2159: 'subject.', 2160: 'submitted', 2161: 'submitting', 2162: 'suburbs', 2163: 'succeed.', 2164: 'success.', 2165: 'such', 2166: 'sugar', 2167: 'sugar.', 2168: 'suggestion.', 2169: 'suggestions.', 2170: 'suitcase', 2171: 'suitcases?', 2172: 'suits', 2173: 'summary', 2174: 'summer', 2175: 'summer.', 2176: 'sun', 2177: 'sunday.', 2178: 'sunday?', 2179: 'sunglasses', 2180: 'sunglasses.', 2181: 'sunny', 2182: 'sunscreen.', 2183: 'supermarket', 2184: 'supper', 2185: 'supplies', 2186: 'supply.', 2187: 'suppose', 2188: 'supposed', 2189: 'sure', 2190: 'sure.', 2191: 'surgery.', 2192: 'surprised', 2193: 'survive.', 2194: 'suspect', 2195: 'suspended.', 2196: 'suspension', 2197: 'swear', 2198: 'sweater.', 2199: 'sweet.', 2200: 'swim', 2201: 'swim.', 2202: 'swimming', 2203: 'swimming.', 2204: 'switzerland.', 2205: 'system.', 2206: 'table.', 2207: 'tag.', 2208: 'tail', 2209: 'tails.', 2210: 'take', 2211: 'take.', 2212: 'taken', 2213: 'taken?', 2214: 'taking', 2215: 'talented', 2216: 'talented,', 2217: 'talented.', 2218: 'talented?', 2219: 'talk', 2220: 'talk.', 2221: 'talkative.', 2222: 'talking', 2223: 'talking.', 2224: 'tall', 2225: 'tall.', 2226: 'taller,', 2227: 'taller.', 2228: 'tallest', 2229: 'tattoo?', 2230: 'taught', 2231: 'tax.', 2232: 'taxi', 2233: 'tea', 2234: 'tea,', 2235: 'tea.', 2236: 'teach', 2237: 'teacher', 2238: 'teacher.', 2239: 'teacher?', 2240: 'teachers.', 2241: 'teaches', 2242: 'teaching', 2243: 'team', 2244: 'team.', 2245: 'teammate.', 2246: 'teams', 2247: 'teenager.', 2248: 'television.', 2249: 'televisions', 2250: 'tell', 2251: 'telling', 2252: 'tells', 2253: 'temporary.', 2254: 'ten', 2255: 'tends', 2256: 'tennis', 2257: 'terrible', 2258: 'terribly', 2259: 'terrified.', 2260: 'tests.', 2261: 'texas.', 2262: 'th.', 2263: 'thailand.', 2264: 'than', 2265: 'thank', 2266: 'thanks', 2267: 'that', 2268: 'that,', 2269: 'that.', 2270: 'that?', 2271: 'thatll', 2272: 'thats', 2273: 'the', 2274: 'theater.', 2275: 'theft.', 2276: 'them', 2277: 'them.', 2278: 'then', 2279: 'then.', 2280: 'there', 2281: 'there,', 2282: 'there.', 2283: 'there?', 2284: 'theres', 2285: 'these', 2286: 'these.', 2287: 'they', 2288: 'they?', 2289: 'theyll', 2290: 'theyre', 2291: 'theyve', 2292: 'thick', 2293: 'thing', 2294: 'thing.', 2295: 'thing?', 2296: 'things', 2297: 'things.', 2298: 'things?', 2299: 'think', 2300: 'think,', 2301: 'think.', 2302: 'think?', 2303: 'thinker.', 2304: 'thinking', 2305: 'thinks', 2306: 'third', 2307: 'thirsty.', 2308: 'thirteen', 2309: 'thirteen.', 2310: 'thirty', 2311: 'thirty.', 2312: 'this', 2313: 'this.', 2314: 'this?', 2315: 'thisll', 2316: 'those', 2317: 'those.', 2318: 'thought', 2319: 'thought.', 2320: 'thousand', 2321: 'threatened', 2322: 'threatened.', 2323: 'three', 2324: 'three-year-old', 2325: 'threw', 2326: 'throw', 2327: 'ticket.', 2328: 'ticket?', 2329: 'tickets', 2330: 'tie', 2331: 'tie.', 2332: 'tight-fitting', 2333: 'tiki.', 2334: 'time', 2335: 'time.', 2336: 'time?', 2337: 'times', 2338: 'tired', 2339: 'tired.', 2340: 'title,', 2341: 'to', 2342: 'to,', 2343: 'to.', 2344: 'to?', 2345: 'today', 2346: 'today,', 2347: 'today.', 2348: 'today?', 2349: 'todays', 2350: 'toe', 2351: 'together', 2352: 'together.', 2353: 'together?', 2354: 'toilet?', 2355: 'tokyo', 2356: 'tokyo.', 2357: 'told', 2358: 'tom', 2359: 'tom,', 2360: 'tom.', 2361: 'tom?', 2362: 'tomatoes.', 2363: 'tomorrow', 2364: 'tomorrow.', 2365: 'tomorrow?', 2366: 'tomorrows', 2367: 'toms', 2368: 'toms.', 2369: 'toms?', 2370: 'tonight.', 2371: 'tonight?', 2372: 'too', 2373: 'too.', 2374: 'too?', 2375: 'took', 2376: 'toolbox', 2377: 'top', 2378: 'tossed', 2379: 'totally', 2380: 'touch', 2381: 'touching?', 2382: 'tour', 2383: 'toward', 2384: 'town', 2385: 'town.', 2386: 'townie?', 2387: 'toy', 2388: 'toys.', 2389: 'train', 2390: 'train.', 2391: 'trainer.', 2392: 'translate.', 2393: 'translated', 2394: 'translator.', 2395: 'travel', 2396: 'tree.', 2397: 'tried', 2398: 'trip', 2399: 'trip?', 2400: 'trouble', 2401: 'trouble.', 2402: 'true.', 2403: 'truly', 2404: 'trunk.', 2405: 'trust', 2406: 'trust.', 2407: 'truth', 2408: 'truthful?', 2409: 'try', 2410: 'try.', 2411: 'trying', 2412: 'tuesday.', 2413: 'tulips,', 2414: 'tune.', 2415: 'turn', 2416: 'turned', 2417: 'tv', 2418: 'tv.', 2419: 'twelve.', 2420: 'twice', 2421: 'twice.', 2422: 'two', 2423: 'two.', 2424: 'ugly', 2425: 'ukraine', 2426: 'ukraine.', 2427: 'ulaanbaatar', 2428: 'umbrella', 2429: 'umbrella,', 2430: 'umbrella.', 2431: 'umbrellas.', 2432: 'uncomfortable.', 2433: 'unconscious.', 2434: 'unconstitutional?', 2435: 'under', 2436: 'underestimate', 2437: 'understand', 2438: 'understand.', 2439: 'understands', 2440: 'understands?', 2441: 'understood', 2442: 'unemployed.', 2443: 'unethical', 2444: 'unhappy,', 2445: 'united', 2446: 'unlocked.', 2447: 'unlucky.', 2448: 'unmarried.', 2449: 'unprofessional.', 2450: 'unsociable.', 2451: 'until', 2452: 'up', 2453: 'up,', 2454: 'up.', 2455: 'upset', 2456: 'upset.', 2457: 'upstairs.', 2458: 'urged', 2459: 'uruguay.', 2460: 'us', 2461: 'us,', 2462: 'us.', 2463: 'use', 2464: 'use?', 2465: 'used', 2466: 'usually', 2467: 'vacation?', 2468: 'van.', 2469: 'vegetables.', 2470: 'very', 2471: 'video', 2472: 'video.', 2473: 'view.', 2474: 'vinegar.', 2475: 'violence', 2476: 'violent.', 2477: 'violin', 2478: 'violin.', 2479: 'visit', 2480: 'visit.', 2481: 'visited', 2482: 'vitamin', 2483: 'volunteer.', 2484: 'vomiting.', 2485: 'wait', 2486: 'wait.', 2487: 'wait?', 2488: 'waited', 2489: 'waiter', 2490: 'waiting', 2491: 'waking', 2492: 'walk', 2493: 'walk.', 2494: 'walked', 2495: 'walking', 2496: 'wall.', 2497: 'wallet', 2498: 'waned', 2499: 'want', 2500: 'want.', 2501: 'wanted', 2502: 'wanted.', 2503: 'wants', 2504: 'wants.', 2505: 'warsaw.', 2506: 'was', 2507: 'was.', 2508: 'wash', 2509: 'washed', 2510: 'washing', 2511: 'washing?', 2512: 'wasnt', 2513: 'waste', 2514: 'watch', 2515: 'watch.', 2516: 'watched', 2517: 'watches', 2518: 'watching', 2519: 'water', 2520: 'water.', 2521: 'water?', 2522: 'waxed', 2523: 'way', 2524: 'we', 2525: 'wealthy.', 2526: 'wear', 2527: 'wearing', 2528: 'wearing?', 2529: 'wears', 2530: 'wears.', 2531: 'weather', 2532: 'weather.', 2533: 'weathers', 2534: 'website', 2535: 'wed', 2536: 'week', 2537: 'week.', 2538: 'weeks', 2539: 'weighed', 2540: 'weight.', 2541: 'welcome', 2542: 'well', 2543: 'well,', 2544: 'well.', 2545: 'went', 2546: 'went,', 2547: 'went.', 2548: 'were', 2549: 'werent', 2550: 'weve', 2551: 'whale', 2552: 'what', 2553: 'whatever', 2554: 'whatll', 2555: 'whats', 2556: 'wheel?', 2557: 'wheels.', 2558: 'when', 2559: 'whenever', 2560: 'whens', 2561: 'where', 2562: 'wheres', 2563: 'whether', 2564: 'which', 2565: 'while', 2566: 'while.', 2567: 'whistling', 2568: 'white', 2569: 'who', 2570: 'who?', 2571: 'whoever', 2572: 'whole', 2573: 'whos', 2574: 'whose', 2575: 'why', 2576: 'wide', 2577: 'wife,', 2578: 'wife.', 2579: 'wifes', 2580: 'will', 2581: 'willing', 2582: 'win.', 2583: 'windmills,', 2584: 'window.', 2585: 'windows.', 2586: 'windsurfing.', 2587: 'wine', 2588: 'wine.', 2589: 'winning', 2590: 'wins.', 2591: 'winter.', 2592: 'winter?', 2593: 'wish', 2594: 'with', 2595: 'without', 2596: 'wolves', 2597: 'woman', 2598: 'woman,', 2599: 'woman.', 2600: 'woman?', 2601: 'women', 2602: 'women,', 2603: 'women.', 2604: 'won', 2605: 'wonder', 2606: 'wondering', 2607: 'wont', 2608: 'wooden', 2609: 'word', 2610: 'word.', 2611: 'words', 2612: 'words.', 2613: 'wore', 2614: 'work', 2615: 'work.', 2616: 'work?', 2617: 'worked', 2618: 'working', 2619: 'working.', 2620: 'works', 2621: 'world', 2622: 'world.', 2623: 'world?', 2624: 'worlds', 2625: 'worried', 2626: 'worried.', 2627: 'worry', 2628: 'worry.', 2629: 'worth', 2630: 'would', 2631: 'wouldnt', 2632: 'wouldve', 2633: 'wrestling', 2634: 'write', 2635: 'write.', 2636: 'writes', 2637: 'writing', 2638: 'written', 2639: 'wrong', 2640: 'wrong.', 2641: 'wrote', 2642: 'yeah.', 2643: 'year,', 2644: 'year.', 2645: 'year?', 2646: 'years', 2647: 'years.', 2648: 'yell', 2649: 'yelling', 2650: 'yellow.', 2651: 'yen.', 2652: 'yesterday', 2653: 'yesterday.', 2654: 'yesterday?', 2655: 'yet', 2656: 'yet.', 2657: 'yet?', 2658: 'yogurt.', 2659: 'yolks.', 2660: 'you', 2661: 'you,', 2662: 'you.', 2663: 'you?', 2664: 'youd', 2665: 'youll', 2666: 'young', 2667: 'young,', 2668: 'younger', 2669: 'younger,', 2670: 'youngest.', 2671: 'your', 2672: 'youre', 2673: 'yours.', 2674: 'yours?', 2675: 'yourself', 2676: 'yourself.', 2677: 'youve', 2678: 'zealand.', 2679: 'zero', 2680: 'zoologist.', 2681: 'zoology', 2682: '€'}\n",
            "3271\n"
          ]
        }
      ],
      "source": [
        "# Define special tokens\n",
        "PAD_TOKEN = '<PAD>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "# Create word-to-index dictionaries\n",
        "source_word2idx = {PAD_TOKEN: 0, UNK_TOKEN: 1} | dict([(word, i+2) for i, word in enumerate(source_words)])\n",
        "target_word2idx = {PAD_TOKEN: 0, UNK_TOKEN: 1} | dict([(word, i+2) for i, word in enumerate(target_words)])\n",
        "\n",
        "# Create index-to-word dictionaries\n",
        "source_idx2word = {i: word for word, i in source_word2idx.items()}\n",
        "target_idx2word = {i: word for word, i in target_word2idx.items()}\n",
        "\n",
        "# Check if the dictionaries have been properly created\n",
        "print(source_word2idx)\n",
        "print(len(target_word2idx))\n",
        "print(source_idx2word)\n",
        "print(len(target_idx2word))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d786efa-0e1c-4cbf-9361-6719d2505634",
      "metadata": {
        "id": "0d786efa-0e1c-4cbf-9361-6719d2505634"
      },
      "source": [
        "### Shuffle and Split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "5f73e42d-b39b-4f8d-beac-19996b41930e",
      "metadata": {
        "id": "5f73e42d-b39b-4f8d-beac-19996b41930e",
        "outputId": "3cb8e620-74d7-44a6-b982-1e8de518e1bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2420,), (269,))"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "#Shuffle the data\n",
        "lines = shuffle(cleaned_data)\n",
        "# Train - Test Split\n",
        "X, y = cleaned_data.cleaned_source, cleaned_data.cleaned_target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "1f90c1fa-6ab1-4e7e-b836-ea08d87b4afc",
      "metadata": {
        "id": "1f90c1fa-6ab1-4e7e-b836-ea08d87b4afc",
        "outputId": "12319f6b-6619-4ad6-a8fa-c63665953c83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3271\n"
          ]
        }
      ],
      "source": [
        "# Input tokens for encoder\n",
        "num_encoder_tokens=len(source_word2idx)\n",
        "# Input tokens for decoder zero padded\n",
        "num_decoder_tokens=len(target_idx2word)\n",
        "print(num_decoder_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9808e34-c5e8-4783-b31d-8d9bb9bd8a46",
      "metadata": {
        "id": "b9808e34-c5e8-4783-b31d-8d9bb9bd8a46"
      },
      "source": [
        "## Generate in Batch:\n",
        "\n",
        "To manage our memory we will create and input data pipeline in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "d200b9b3-1c54-4880-a53e-1884edcf67ad",
      "metadata": {
        "id": "d200b9b3-1c54-4880-a53e-1884edcf67ad"
      },
      "outputs": [],
      "source": [
        "def generate_batch(X, y, batch_size=128):\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "            batch_X = X[j:j + batch_size]\n",
        "            batch_y = y[j:j + batch_size]\n",
        "\n",
        "            encoder_input_data = np.zeros((len(batch_X), max_source_length), dtype='float32')\n",
        "            decoder_input_data = np.zeros((len(batch_X), max_target_length), dtype='float32')\n",
        "            decoder_target_data = np.zeros((len(batch_X), max_target_length, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "            for i, (input_text, target_text) in enumerate(zip(batch_X, batch_y)):\n",
        "                input_seq = [source_word2idx.get(word, source_word2idx[UNK_TOKEN]) for word in input_text.split()]\n",
        "                target_seq = [target_word2idx.get(word, target_word2idx[UNK_TOKEN]) for word in target_text.split()]\n",
        "\n",
        "                encoder_input_data[i] = pad_sequences([input_seq], maxlen=max_source_length, padding='post')[0]\n",
        "                decoder_input_data[i] = pad_sequences([target_seq], maxlen=max_target_length, padding='post')[0]\n",
        "\n",
        "                for t in range(1, len(target_seq)):\n",
        "                    decoder_target_data[i, t - 1, target_seq[t]] = 1.\n",
        "\n",
        "            # Yield as expected structure: ((inputs), targets)\n",
        "            yield ((encoder_input_data, decoder_input_data), decoder_target_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "73f32bb8-7d3d-407d-aa94-b5a6042eb108",
      "metadata": {
        "id": "73f32bb8-7d3d-407d-aa94-b5a6042eb108"
      },
      "outputs": [],
      "source": [
        "def create_tf_dataset(X, y, batch_size=128):\n",
        "    output_signature = (\n",
        "        (tf.TensorSpec(shape=(None, max_source_length), dtype=tf.float32),  # encoder_input_data\n",
        "         tf.TensorSpec(shape=(None, max_target_length), dtype=tf.float32)),  # decoder_input_data\n",
        "        tf.TensorSpec(shape=(None, max_target_length, num_decoder_tokens), dtype=tf.float32)  # decoder_target_data\n",
        "    )\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        lambda: generate_batch(X, y, batch_size),  # Lambda to call the generator function\n",
        "        output_signature=output_signature  # Defining the output signature for the dataset\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7baee849-605e-455e-8e1b-fba2ae5b0217",
      "metadata": {
        "id": "7baee849-605e-455e-8e1b-fba2ae5b0217"
      },
      "source": [
        "# Model Building:\n",
        "\n",
        "1. encoder inputs: The 2D array will be of shape (batch_size, max source sentence length). For a batch_size of 128 and a max source sentence length of 47, the shape of encoder_input will be (128,47)\n",
        "\n",
        "2. decoder inputs: The 2D array will be of shape (batch_size, max target sentence length). For a batch_size of 128 and a max target sentence length of 55, the shape of decoder inputs will be (128,55)\n",
        "\n",
        "3. decoder outputs: The 3D array will be of shape (batch_size, max target sentence length, number of unique words in target sentences). For a batch_size of 128 and a max target sentence length of 55, the shape of decoder output will be (128,55, 27200)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd4f1455-3246-4475-b2bf-3427b6485d24",
      "metadata": {
        "id": "dd4f1455-3246-4475-b2bf-3427b6485d24"
      },
      "source": [
        "## Encoder Architecture:\n",
        "Encoder encodes the input sentence.\n",
        "1. It takes the input source tokens from input layer.\n",
        "2. Embedding layer then translates sparse vectors into a dense lower dimesional space preserving teh semantic realtionships.\n",
        "3. Create the LSTM layer and only set return_state to True, because we want hidden state and cell state, as an input to decoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "a0861241-cb71-4d11-ace0-4ec1a7f8d195",
      "metadata": {
        "id": "a0861241-cb71-4d11-ace0-4ec1a7f8d195"
      },
      "outputs": [],
      "source": [
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "latent_dim=256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "6e552c51-2633-4934-8d8b-2447c259b948",
      "metadata": {
        "id": "6e552c51-2633-4934-8d8b-2447c259b948"
      },
      "outputs": [],
      "source": [
        "def define_encoder(input_shape, num_encoder_tokens, latent_dim):\n",
        "    \"\"\"\n",
        "    Defines the encoder architecture for a sequence-to-sequence model.\n",
        "\n",
        "    The encoder processes input sequences through an embedding layer and LSTM,\n",
        "    returning the final states that capture the encoded information.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        Shape of the input tensor (max_sequence_length,) for variable-length sequences\n",
        "    num_encoder_tokens : int\n",
        "        Size of the source vocabulary (including special tokens)\n",
        "    latent_dim : int\n",
        "        Dimensionality of the embedding and LSTM layers\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (encoder_inputs, encoder_states)\n",
        "        encoder_inputs : keras.Input\n",
        "            Input layer for the encoder\n",
        "        encoder_states : list\n",
        "            Final states [hidden_state, cell_state] from the LSTM\n",
        "    \"\"\"\n",
        "    encoder_inputs = Input(shape=input_shape, name='encoder_inputs')\n",
        "    enc_emb = Embedding(num_encoder_tokens, latent_dim, mask_zero=True, name='encoder_embedding')(encoder_inputs)\n",
        "    encoder_lstm = LSTM(latent_dim, return_state=True, name='encoder_lstm')\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    return encoder_inputs, encoder_states"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a588736-5e32-441d-aa73-383641145405",
      "metadata": {
        "id": "9a588736-5e32-441d-aa73-383641145405"
      },
      "source": [
        "## Decoder Architecture.\n",
        "\n",
        "1. Decoder uses hidden state and cell state from encoder and from embedding layer as an input.\n",
        "\n",
        "2. Decoder returns output sentence and also hidden and cell states.\n",
        "\n",
        "3. The final layer in decoder is linear layer(dense) with softmax activation function used for predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "8fa93f9f-f1e2-403a-b44a-e3a89e786b72",
      "metadata": {
        "id": "8fa93f9f-f1e2-403a-b44a-e3a89e786b72"
      },
      "outputs": [],
      "source": [
        "def define_decoder(latent_dim, num_decoder_tokens, encoder_states, max_target_length):\n",
        "    \"\"\"\n",
        "    Defines the decoder architecture for a sequence-to-sequence model.\n",
        "\n",
        "    The decoder processes target sequences through an embedding layer and LSTM,\n",
        "    using the encoder states as initial state, and outputs probability distributions\n",
        "    over the target vocabulary via a dense softmax layer.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    latent_dim : int\n",
        "        Dimensionality of the embedding and LSTM layers (must match encoder)\n",
        "    num_decoder_tokens : int\n",
        "        Size of the target vocabulary (including special tokens)\n",
        "    encoder_states : list\n",
        "        Final states [hidden_state, cell_state] from the encoder LSTM\n",
        "    max_target_length : int\n",
        "        Maximum length of target sequences (for shape reference)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple: (decoder_inputs, decoder_outputs)\n",
        "        decoder_inputs : keras.Input\n",
        "            Input layer for the decoder (teacher forcing inputs)\n",
        "        decoder_outputs : keras.Layer\n",
        "            Output tensor containing sequence of vocabulary probabilities\n",
        "    \"\"\"\n",
        "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
        "    dec_emb_layer = Embedding(num_decoder_tokens, latent_dim, mask_zero=True, name='decoder_embedding')\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "    decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
        "    decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
        "\n",
        "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "    return decoder_inputs, decoder_outputs, dec_emb_layer, decoder_lstm, decoder_dense"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab5d5cf2-e568-4b6f-b340-47f0b1f7ebc1",
      "metadata": {
        "id": "ab5d5cf2-e568-4b6f-b340-47f0b1f7ebc1"
      },
      "source": [
        "### Building Seq - to - Seq Model:\n",
        "This function creates a complete model that:\n",
        "\n",
        "1. Encodes input sequences into context vectors\n",
        "2. Decodes the context vectors into target sequences\n",
        "3. Outputs probability distributions over the target vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "9fbf8daa-9141-4ff4-bdcc-6f24b25baa8c",
      "metadata": {
        "id": "9fbf8daa-9141-4ff4-bdcc-6f24b25baa8c"
      },
      "outputs": [],
      "source": [
        "def build_seq2seq_model(input_shape, num_encoder_tokens, num_decoder_tokens, latent_dim, max_target_length):\n",
        "    \"\"\"\n",
        "    Constructs an end-to-end sequence-to-sequence model combining encoder and decoder.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_shape : tuple\n",
        "        Shape of the input sequences (max_sequence_length,)\n",
        "    num_encoder_tokens : int\n",
        "        Size of the source vocabulary (including special tokens)\n",
        "    num_decoder_tokens : int\n",
        "        Size of the target vocabulary (including special tokens)\n",
        "    latent_dim : int\n",
        "        Dimensionality of the embedding and LSTM layers\n",
        "    max_target_length : int\n",
        "        Maximum length of target sequences (for reference)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    keras.Model\n",
        "        A compiled seq2seq model with encoder and decoder components\n",
        "    \"\"\"\n",
        "    encoder_inputs, encoder_states = define_encoder(input_shape, num_encoder_tokens, latent_dim)\n",
        "    decoder_inputs, decoder_outputs, dec_emb_layer, decoder_lstm, decoder_dense = define_decoder(latent_dim, num_decoder_tokens, encoder_states, max_target_length)\n",
        "\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "    return model, encoder_inputs, decoder_inputs, encoder_states, dec_emb_layer, decoder_lstm, decoder_dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "44599295-ce6c-4218-a05f-eccbca87f026",
      "metadata": {
        "id": "44599295-ce6c-4218-a05f-eccbca87f026",
        "outputId": "c687d0c6-e67c-48f0-ce72-9f724beba188",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input data shape: (32, 25), (32, 22)\n",
            "Target data shape: (32, 22, 3271)\n"
          ]
        }
      ],
      "source": [
        "# Create dataset\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
        "\n",
        "train_dataset = create_tf_dataset(X_train, y_train, batch_size=batch_size)\n",
        "val_dataset = create_tf_dataset(X_test, y_test, batch_size=batch_size)\n",
        "\n",
        "# Test the first batch\n",
        "for batch in train_dataset.take(1):\n",
        "    print(f\"Input data shape: {batch[0][0].shape}, {batch[0][1].shape}\")  # encoder_input_data, decoder_input_data\n",
        "    print(f\"Target data shape: {batch[1].shape}\")  # decoder_target_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "62466cfc-db7a-4eaf-8991-b57b8f8cd8fe",
      "metadata": {
        "id": "62466cfc-db7a-4eaf-8991-b57b8f8cd8fe",
        "outputId": "bd90d5bb-ac3b-4019-f7be-f08436d97866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m256\u001b[0m)   │    \u001b[38;5;34m686,848\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_3         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m) │    \u001b[38;5;34m837,376\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m525,312\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │ not_equal_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m525,312\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m840,647\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │ \u001b[38;5;34m3271\u001b[0m)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">686,848</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ not_equal_3         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">837,376</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ encoder_embeddin… │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │ not_equal_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_dense       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">840,647</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3271</span>)             │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,415,495\u001b[0m (13.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,415,495</span> (13.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,415,495\u001b[0m (13.03 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,415,495</span> (13.03 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Prepare the dataset for training\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)  # Optimizing for performance\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Build and compile the model\n",
        "input_shape = (None,)  # Variable-length input sequence (e.g., (None,))\n",
        "latent_dim = 256  # Latent dimension for LSTM\n",
        "model, encoder_inputs, decoder_inputs, encoder_states, dec_emb_layer, decoder_lstm, decoder_dense = build_seq2seq_model(\n",
        "    input_shape=(max_source_length,),\n",
        "    num_encoder_tokens=num_encoder_tokens,\n",
        "    num_decoder_tokens=num_decoder_tokens,\n",
        "    latent_dim=latent_dim,\n",
        "    max_target_length=max_target_length\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "ae7d5f4d-580d-4d2f-a911-1fe8510801da",
      "metadata": {
        "id": "ae7d5f4d-580d-4d2f-a911-1fe8510801da",
        "outputId": "fd147293-e4fc-4fd3-f41c-ef6e24f86bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.0437 - loss: 6.1739\n",
            "Epoch 1: val_accuracy improved from -inf to 0.05433, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - accuracy: 0.0440 - loss: 6.1506 - val_accuracy: 0.0543 - val_loss: 5.0801\n",
            "Epoch 2/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0546 - loss: 4.8939\n",
            "Epoch 2: val_accuracy improved from 0.05433 to 0.05522, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.0546 - loss: 4.8897 - val_accuracy: 0.0552 - val_loss: 4.9793\n",
            "Epoch 3/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.0556 - loss: 4.6676\n",
            "Epoch 3: val_accuracy improved from 0.05522 to 0.05682, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.0556 - loss: 4.6645 - val_accuracy: 0.0568 - val_loss: 4.9326\n",
            "Epoch 4/100\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0578 - loss: 4.4755\n",
            "Epoch 4: val_accuracy improved from 0.05682 to 0.05859, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.0578 - loss: 4.4747 - val_accuracy: 0.0586 - val_loss: 4.8479\n",
            "Epoch 5/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0623 - loss: 4.2739\n",
            "Epoch 5: val_accuracy improved from 0.05859 to 0.06286, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.0624 - loss: 4.2715 - val_accuracy: 0.0629 - val_loss: 4.7494\n",
            "Epoch 6/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0709 - loss: 4.0728\n",
            "Epoch 6: val_accuracy improved from 0.06286 to 0.06463, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.0710 - loss: 4.0701 - val_accuracy: 0.0646 - val_loss: 4.6866\n",
            "Epoch 7/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0776 - loss: 3.8757\n",
            "Epoch 7: val_accuracy improved from 0.06463 to 0.06516, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.0776 - loss: 3.8735 - val_accuracy: 0.0652 - val_loss: 4.6591\n",
            "Epoch 8/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0816 - loss: 3.6941\n",
            "Epoch 8: val_accuracy improved from 0.06516 to 0.06623, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.0817 - loss: 3.6921 - val_accuracy: 0.0662 - val_loss: 4.6453\n",
            "Epoch 9/100\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.0873 - loss: 3.5202\n",
            "Epoch 9: val_accuracy improved from 0.06623 to 0.07013, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.0873 - loss: 3.5187 - val_accuracy: 0.0701 - val_loss: 4.6288\n",
            "Epoch 10/100\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.0920 - loss: 3.3462\n",
            "Epoch 10: val_accuracy improved from 0.07013 to 0.07422, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.0920 - loss: 3.3449 - val_accuracy: 0.0742 - val_loss: 4.6230\n",
            "Epoch 11/100\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.0973 - loss: 3.2042\n",
            "Epoch 11: val_accuracy improved from 0.07422 to 0.07564, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step - accuracy: 0.0973 - loss: 3.2033 - val_accuracy: 0.0756 - val_loss: 4.6262\n",
            "Epoch 12/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1028 - loss: 3.0111\n",
            "Epoch 12: val_accuracy improved from 0.07564 to 0.07848, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.1029 - loss: 3.0086 - val_accuracy: 0.0785 - val_loss: 4.6387\n",
            "Epoch 13/100\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1098 - loss: 2.8116\n",
            "Epoch 13: val_accuracy improved from 0.07848 to 0.08043, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 35ms/step - accuracy: 0.1098 - loss: 2.8104 - val_accuracy: 0.0804 - val_loss: 4.6721\n",
            "Epoch 14/100\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1165 - loss: 2.6256\n",
            "Epoch 14: val_accuracy did not improve from 0.08043\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - accuracy: 0.1165 - loss: 2.6248 - val_accuracy: 0.0801 - val_loss: 4.7863\n",
            "Epoch 15/100\n",
            "\u001b[1m74/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.1218 - loss: 2.4927\n",
            "Epoch 15: val_accuracy improved from 0.08043 to 0.08381, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step - accuracy: 0.1218 - loss: 2.4914 - val_accuracy: 0.0838 - val_loss: 4.7576\n",
            "Epoch 16/100\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.1294 - loss: 2.3362\n",
            "Epoch 16: val_accuracy improved from 0.08381 to 0.08594, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1294 - loss: 2.3355 - val_accuracy: 0.0859 - val_loss: 4.7492\n",
            "Epoch 17/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1380 - loss: 2.1837\n",
            "Epoch 17: val_accuracy improved from 0.08594 to 0.08647, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.1381 - loss: 2.1821 - val_accuracy: 0.0865 - val_loss: 4.7919\n",
            "Epoch 18/100\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1486 - loss: 2.0178\n",
            "Epoch 18: val_accuracy improved from 0.08647 to 0.08913, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.1486 - loss: 2.0175 - val_accuracy: 0.0891 - val_loss: 4.8607\n",
            "Epoch 19/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1577 - loss: 1.8794\n",
            "Epoch 19: val_accuracy did not improve from 0.08913\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - accuracy: 0.1579 - loss: 1.8785 - val_accuracy: 0.0890 - val_loss: 4.9370\n",
            "Epoch 20/100\n",
            "\u001b[1m73/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1662 - loss: 1.7456\n",
            "Epoch 20: val_accuracy improved from 0.08913 to 0.09002, saving model to best_seq2seq_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.1663 - loss: 1.7447 - val_accuracy: 0.0900 - val_loss: 4.9915\n",
            "Epoch 20: early stopping\n",
            "Restoring model weights from the end of the best epoch: 10.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# 1. Define callbacks\n",
        "callbacks = [\n",
        "    # Stop training if val_loss doesn't improve for 3 consecutive epochs\n",
        "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1),\n",
        "\n",
        "    # Save the model with the best validation accuracy\n",
        "    ModelCheckpoint('best_seq2seq_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
        "]\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 3. Training parameters\n",
        "train_samples = len(X_train)\n",
        "val_samples = len(X_test)\n",
        "\n",
        "steps_per_epoch = train_samples // batch_size\n",
        "validation_steps = val_samples // batch_size\n",
        "\n",
        "# 4. Train the model with callbacks\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    validation_data=val_dataset,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 5. Save final model (optional, in case best wasn't triggered)\n",
        "model.save('final_seq2seq_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "7d72a464-fffb-4dbe-8046-84b530eaf80b",
      "metadata": {
        "id": "7d72a464-fffb-4dbe-8046-84b530eaf80b"
      },
      "outputs": [],
      "source": [
        "# Encoder model (same as in training)\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Decoder setup for inference\n",
        "# These inputs will hold the LSTM states for each timestep\n",
        "decoder_state_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
        "decoder_state_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Embedding layer reused from training\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Reuse the LSTM layer and pass in the previous states\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(\n",
        "    dec_emb2, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states2 = [state_h2, state_c2]\n",
        "\n",
        "# Reuse the dense softmax layer\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final inference decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs2] + decoder_states2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1a049c9-f4b2-4484-b44b-0c8066d99449",
      "metadata": {
        "id": "a1a049c9-f4b2-4484-b44b-0c8066d99449"
      },
      "source": [
        "### Function for Quick Predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "1e519462-b1f7-4f89-acfc-241b278dc8fb",
      "metadata": {
        "id": "1e519462-b1f7-4f89-acfc-241b278dc8fb"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of\n",
        "    #target sequence with the start character.\n",
        "    target_seq[0, 0] = target_word2idx['START_']\n",
        "# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "# Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word =target_idx2word[sampled_token_index]\n",
        "        decoded_sentence += ' '+ sampled_word\n",
        "# Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_word == '_END' or\n",
        "           len(decoded_sentence) > 50):\n",
        "            stop_condition = True\n",
        "# Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "# Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "b759353a-4717-4a05-bb23-efa2907d04d3",
      "metadata": {
        "id": "b759353a-4717-4a05-bb23-efa2907d04d3"
      },
      "outputs": [],
      "source": [
        "train_gen = generate_batch(X_train, y_train, batch_size=1)\n",
        "k = -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "b20931ac-9d15-4303-a97b-a2bcf1f630c4",
      "metadata": {
        "id": "b20931ac-9d15-4303-a97b-a2bcf1f630c4",
        "outputId": "f53aacb3-f166-4e0d-ca53-bc09c5a940e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "Input Source sentence: i like white roses better than red ones.\n",
            "Actual Target Translation:  मलाई रातो गुलाब भन्दा सेतो गुलाब मन पर्छ। \n",
            "Predicted Target Translation:  मलाई थाहा छ मलाई थाहा छ टम गर्न पर्छ। \n"
          ]
        }
      ],
      "source": [
        "k+=1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input Source sentence:', X_train[k:k+1].values[0])\n",
        "print('Actual Target Translation:', y_train[k:k+1].values[0][6:-4])\n",
        "print('Predicted Target Translation:', decoded_sentence[:-4])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnFW7I-LylQh",
        "outputId": "6d40fed9-1165-40e4-a865-ab5617e39891"
      },
      "id": "TnFW7I-LylQh",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "d33c9b53-00c9-4774-be25-7f56dc83f6f5",
      "metadata": {
        "id": "d33c9b53-00c9-4774-be25-7f56dc83f6f5",
        "outputId": "dd7228bc-713e-4fe7-c6d4-7fccfa5a2ecd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://2008a4e472f37908c0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# ---- Helper Function to Decode Sequences ----\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1 with only the start token.\n",
        "    target_seq = np.zeros((1, 1), dtype='int32')\n",
        "    target_seq[0, 0] = target_word2idx.get('<PAD>')  # or <START> if you have one\n",
        "\n",
        "    # Output sequence\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_idx2word.get(sampled_token_index, '<UNK>')\n",
        "\n",
        "        if sampled_word == '<PAD>' or len(decoded_sentence.split()) > max_target_length:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += ' ' + sampled_word\n",
        "\n",
        "            # Update target sequence (of length 1)\n",
        "            target_seq = np.zeros((1, 1), dtype='int32')\n",
        "            target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "            # Update states\n",
        "            states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "# ---- Preprocessing for the input sentence ----\n",
        "def preprocess_input(sentence):\n",
        "    sequence = [source_word2idx.get(word, source_word2idx['<UNK>']) for word in sentence.lower().split()]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_source_length, padding='post')\n",
        "    return sequence\n",
        "\n",
        "# ---- Main function for Gradio ----\n",
        "def translate_sentence(input_sentence):\n",
        "    preprocessed_input = preprocess_input(input_sentence)\n",
        "    translated_output = decode_sequence(preprocessed_input)\n",
        "    return translated_output\n",
        "\n",
        "# ---- Build Gradio Interface ----\n",
        "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"blue\", neutral_hue=\"blue\")) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"<h1 style='text-align: center; color: #00BFFF;'>Seq2Seq Translator</h1>\n",
        "        <h3 style='text-align: center; color: white;'>Enter a sentence to translate</h3>\"\"\",\n",
        "    )\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_text = gr.Textbox(label=\"Input Sentence\", placeholder=\"Type your source sentence here...\")\n",
        "            translate_button = gr.Button(\"Translate\", elem_id=\"translate-btn\")\n",
        "        with gr.Column():\n",
        "            output_text = gr.Textbox(label=\"Translated Sentence\", placeholder=\"Translation will appear here...\")\n",
        "\n",
        "    translate_button.click(fn=translate_sentence, inputs=input_text, outputs=output_text)\n",
        "\n",
        "# ---- Custom CSS to set background ----\n",
        "demo.launch(share=True, inline=False, server_name=\"0.0.0.0\", server_port=7960)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}